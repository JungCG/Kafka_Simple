# Kafka The Definitive Guide
1. **Kafka**-related **Basic Knowledge** (Summary of contents of **Kafka The Definitive Guide**)
2. **Simple Projects** (Source code based on [AndersonChoi' lecture](https://github.com/AndersonChoi/tacademy-kafka))

## Contents
1. [Using](#using)
2. [A quick look at Kafka](#a-quick-look-at-kafka)
3. [Kafka installation and configuration](#kafka-installation-and-configuration)
4. [Kafka Producer](#kafka-producer)
5. [Kafka Consumer](#kafka-consumer)
6. [Kafka internal mechanism](#kafka-internal-mechanism)
7. [Reliable data delivery](#reliable-data-delivery)
8. [Build the data pipeline](#build-the-data-pipeline)
9. [Cross-cluster data mirroring](#cross-cluster-data-mirroring)
10. [Kafka management](#kafka-management)
11. [Kafka monitoring](#kafka-monitoring)
12. [Stream processing](#stream-processing)
13. [Simple Projects](#simple-projects)

---------------------------------------------------

## Using
1. BackEnd - Java(JDK 1.8), **Kafka(v2.5.0)**, **Zookeeper**
2. OS - Ubuntu 20.04.1 LTS (VMware), Amazon Linux 2(AWS EC2)
3. IDE - IntelliJ(Community / 11.0.9.1 / 2020.3.2)
4. Server - **AWS(EC2, Route 53)**
5. etc - Homebrew, **telegraf(v1.17.2)**

----------------------------------------------------

## A quick look at Kafka
1. **카프카 (Kafka)**
    - **메시지 발행/구독 시스템**
    - **분산 커밋 로그(distributed commit log)** 또는 분산 스트리밍 플랫폼(distributed streaming platform)
    - 카프카의 데이터도 지속해서 저장하고 읽을 수 있다.
    - 시스템 장애에 대비하고 확장에 따른 성능 저하를 방지하기 위해 데이터가 분산 처리될 수 있다.
2. **메시지 (message)**
    - **데이터의 기본 단위** (이것은 데이터베이스의 행(row)이나 레코드(record)에 비유될 수 있다.)
    - 카프카는 **메시지를 바이트 배열의 데이터로 간주**하므로 특정 형식이나 의미를 갖지 않는다.
    - 메시지에는 **키**(key)라는 메타데이터가 포함될 수 있으며, 이것 역시 바이트 배열이고 특별한 의미를 갖지 않는다.
    - 카프카의 메시지 데이터는 **토픽**(topic)으로 분류된 **파티션(partition)에 수록**되는데, **이때 데이터를 수록할 파티션을 결정하기 위해 일관된 해시 값으로 키를 생성**한다.
    - **따라서 같은 키 값을 갖는 메시지는 항상 같은 파티션에 수록된다.**
3. **배치 (batch)**
    - 카프카는 효율성을 위해서 여러 개의 메시지를 모아 배치 형태로 파티션에 수록하므로 네티워크로부터 매번 각 메시지를 받아서 처리하는 데 따른 부담을 줄일 수 있다.
    - 물론 이 경우 대기 시간(latency)과 처리량(throughput) 간의 트레이트오프가 생길 수 있다.(배치의 크기가 클수록 단위 시간당 처리될 수 있는 메시지는 많아지지만, 각 메시지의 전송 시간은 더 길어진다.)
    - **배치에는 데이터 압축이 적용되므로 더 효율적인 데이터 전송과 저장 능력을 제공한다.**
4. **스키마 (schema)**
    - 카프카는 메시지를 단순히 바이트 배열로 처리하지만, **내용을 이해하기 쉽도록 메시지의 구조를 나타내는 스키마를 사용할 수 있다.**
    - **각 애플리케이션의 필요에 따라 메시지 스키마로 여러 가지 표준 형식을 사용할 수 있다.**
    - 가장 간단한 방법으로는 **JSON**(Javascript Object Notation)이나 **XML**(Extensible Markup Language)이 있으며, 알기 쉽고 사용하기 쉽다. 그러나 **강력한 데이터 타입에 대한 지원이 부족하고 스키마 버전 간의 호환성이 떨어진다.**
    - 따라서 많은 카프카 개발자들이 **아파치 Avro를 선호**한다. 이것은 원래 **하둡**(Hadoop)을 위해 개발된 **직렬화(serializaion) 프레임워크**이다.
    - **Avro는 데이터를 직렬화하는 형식을 제공하며, 메시지와는 별도로 스키마를 유지 관리하므로 스키마가 변경되더라도 애플리케이션의 코드를 추가하거나 변경할 필요가 없다.** 또한, 강력한 데이터 타입을 지원하며 **스키마 신구버전 간의 호환성도 제공**한다.
    - **카프카에서는 일관된 데이터 형식이 중요하다.** 메시지 쓰기와 읽기 작업을 분리해서 할 수 있기 때문이다.
    - 만일 두 작업이 하나로 합쳐져 있다면 구버전과 신버전의 데이터 형식을 병행 처리하기 위해 메시지 구독 애플리케이션이 먼저 업데이트되어야 하며, 그 다음에 메시지 발행 애플리케이션도 업데이트되어야 한다.
    - 그러나 **카프카에서는 잘 정의된 스키마를 공유 리포지토리(repository)에 저장하여 사용할 수 있으므로 애플리케이션 변경 없이 메시지를 처리할 수 있다.**
5. **토픽 (topic)과 파티션 (partition)**
    - 카프카의 **메시지는 토픽으로 분류된다.** (데이터베이스 테이블이나 파일 시스템의 폴더와 유사하다.)
    - **하나의 토픽은 여러 개의 파티션으로 구성**될 수 있다.
    - 커밋 로그 데이터의 관점으로 보면 파티션은 하나의 로그에 해당한다.
    - 메시지는 파티션에 추가되는 형태로만 수록되며, 맨 앞부터 제일 끝까지의 순서로 읽힌다.
    - **대개 하나의 토픽은 여러 개의 파티션을 갖지만, 메시지 처리 순서는 토픽이 아닌 파티션별로 유지 관리된다.**
    - 추가되는 메시지는 각 파티션의 끝에 수록된다.
    - 각 파티션은 서로 다른 서버에 분산될 수 있다. **하나의 토픽이 여러 서버에 걸쳐 수평적으로 확장될 수 있음을 의미하므로 단일 서버로 처리할 때보다 훨씬 성능이 우수하다.**
    - 카프카와 같은 시스템의 데이터를 얘기할 때 **스트림**(stream)이라는 용어가 자주 사용된다.
    - **대부분 스트림은 파티션의 개수와 상관없이 하나의 토픽 데이터로 간주되며, 데이터를 쓰는 프로듀서로부터 데이터를 읽는 컨슈머로 이동되는 연속적인 데이터**를 나타낸다.
    - **카프카 스트림즈(Kafka Streams)**, **아파치 Samza**, **Storm**과 같은 프레임워크에서 실시간으로 메시지를 처리할 때 주로 사용되는 방법이 스트림이다.
    - 이것은 실시간이 아닌 오프라인으로 대량의 데이터를 처리하도록 설계된 프레임워크인 하둡의 방법과 대비된다.
6. **프로듀서 (Producer)와 컨슈머 (Consumer)**
    - **카프카 클라이언트는 시스템의 사용자이며 기본적으로 프로듀서와 컨슈머라는 두 가지 형태가 있다.**
    - 또한, **데이터 통합을 위한 카프카 커넥트(Kafka Connect) API**와 **스트림 처리를 위한 카프카 스트림즈의 클라이언트 API**도 있으며, **이 클라이언트들은 나름의 프로듀서와 컨슈머를 가지고 사용한다.**
    - **프로듀서는 새로운 메시지를 생성한다(쓰기).**
    - **메시지는 특정 토픽으로 생성되며, 기본적으로 프로듀서는 메시지가 어떤 파티션에 수록되는지 관여하지 않는다.**
    - 그러나 **때로는 프로듀서가 특정 파티션에 메시지를 직접 쓰는 경우가 있다.** 이때는 **메시지 키와 파티셔너** (partitioner)를 사용한다.
    - **파티셔너는 키의 해시 값을 생성하고 그것을 특정 파티션에 대응시킴으로써 지정된 키를 갖는 메시지가 항상 같은 파티션에 수록되게 해준다.**
    - **프로듀서는 또한 나름의 커스텀 파티셔너를 갖고 다른 규칙에 따라 메시지가 파티션에 대응되게 할 수도 있다.**
    - **컨슈머는 메시지를 읽는다.**
    - **컨슈머는 하나 이상의 토픽을 구독하여 메시지가 생성된 순서로 읽으며, 메시지의 오프셋(offset)을 유지하여 읽는 메시지의 위치를 알 수 있다.**
    - 또 다른 종류의 메타데이터인 **오프셋**은 지속적으로 증가하는 정숫값이며, 메시지가 생성될 때 **카프카가 추가**해준다.
    - 파티션에 수록된 각 메시지는 고유한 오프셋을 갖는다.
    - 그리고 **주키퍼(Zookeeper)나 카프카에서는 각 파티션에서 마지막에 읽은 메시지의 오프셋을 저장하고 있으므로 컨슈머가 메시지 읽기를 중단했다가 다시 시작하더라도 언제든 그 다음 메시지부터 읽을 수 있다.**
    - **컨슈머는 컨슈머 그룹의 멤버로 동작**한다.
    - **컨슈머 그룹은 하나 이상의 컨슈머로 구성**되며, **한 토픽을 소비하기 위해 같은 그룹의 여러 컨슈머가 함께 동작**한다.
    - 그리고 **한 토픽의 각 파티션은 하나의 컨슈머만 소비할 수 있다.**
    - 각 컨슈머가 특정 파티션에 대응되는 것을 **파티션 소유권**(ownership)이라고 한다.
    - **다량의 메시지를 갖는 토픽을 소비하기 위해 컨슈머를 수평적으로 확장할 수 있다.**
    - **한 컨슈머가 자신의 파티션 메시지를 읽는 데 실패하더라도 같은 그룹의 다른 컨슈머가 파티션 소유권을 재조정받은 후 실패한 컨슈머의 파티션 메시지를 대신 읽을 수 있다.**
7. **브로커(broker)와 클러스터(cluster)**
    - **하나의 카프카 서버를 브로커**라고 한다.
    - **브로커는 프로듀서로부터 메시지를 수신하고 오프셋을 지정한 후 해당 메시지를 디스크에 저장**한다.
    - 또한 컨슈머의 파티션 읽기 요청에 응답하고 디스크에 수록된 메시지를 전송한다.
    - 하나의 브로커는 초당 수천 개의 토픽과 수백만 개의 메시지를 처리할 수 있다.
    - **카프카 브로커는 클러스터의 일부로 동작하도록 설계**되었다.
    - 즉 **여러 개의 브로커가 하나의 클러스터에 포함될 수 있으며, 그 중 하나는 자동으로 선정되는 클러스터 컨트롤러(controller)의 기능을 수행**한다.
    - **컨트롤러는 같은 클러스터의 각 브로커에게 담당 파티션을 할당하고 브로커들이 정상적으로 동작하는지 모니터링하는 관리 기능을 맡는다.**
    - 각 파티션은 클러스터의 한 브로커가 소유하며, 그 브로커를 **파티션 리더**(leader)라고 한다.
    - **같은 파티션이 여러 브로커에 지정될 수도 있는데, 이때는 해당 파티션이 복제(replication)된다.** 이 경우 해당 파티션의 메시지는 중복으로 저장되지만, **관련 브로커에 장애가 생기면 다른 브로커가 소유권을 인계받아 그 파티션을 처리**할 수 있다.
    - **각 파티션을 사용하는 모든 컨슈머와 프로듀서는 파티션 리더에 연결해야 한다.**
8. **보존 (retention)**
    - 아파치 **카프카의 핵심 기능으로 보존**이 있다. 이것은 **일정 기간 메시지를 보존하는 것**이다.
    - 카프카 브로커는 기본적으로 **토픽의 보존 설정을 하도록 구성**되는데 예를 들어, 7일과 같이 **일정 기간** 메시지를 보존하거나 1GB와 같이 **지정된 토픽 크기**가 될 때까지 메시지를 보존할 수 있다.
    - 그리고 이런 **한도 값에 도달하면 최소한의 데이터가 유지되도록 만료 메시지들이 삭제**된다.
    - **메시지가 유용할 때만 보존되도록 토픽마다 보존 설정을 다르게 지정할 수도 있다.**
    - 예를 들어, 지속적인 메시지 유지가 필요한 토픽은 7일 동안 보존하고, 애플리케이션의 성능 관련 메트릭을 저장한 토픽은 수시간 동안만 보존되게 할 수 있다.
    - 또한, **토픽은 압축 로그(log compacted) 설정으로 구성될 수도 있다.**
    - 이 경우 **같은 키를 갖는 메시지들은 가장 최신 것만 보존**된다.
    - 따라서 마지막으로 변경된 것이 중요한 로그 데이터에 유용하다.
9. **다중 클러스터**
    - 카프카가 많이 설치되어 사용될 때 **다중 클러스터(multiple cluster)를 고려하면 다음과 같은 장점**이 있다.
        - **데이터 타입에 따라 구분해서 처리할 수 있음**
        - **보안 요구사항을 분리해서 처리할 수 있음**
        - **재해 복구를 대비한 다중 데이터센터를 유지할 수 있음**
    - 특히 카프카가 다중 데이터센터로 설치되어 동작할 때는 메시지가 데이터센터 간에 복제되어야 하며, 그렇게 하면 온라인 애플리케이션에서는 사용자의 처리 결과를 양쪽 사이트 모두에서 사용할 수 있다.
    - 예를 들어, 한 데이터센터에 접속된 사용자가 자신의 프로파일에 있는 공개 정보를 변경한다면 다른 데이터센터에서도 변경 내용을 볼 수 있게 될 것이다.
    - 또는 여러 사이트에서 수집된 모니터링 데이터를 분석/보안 시스템이 운영되는 센터로 집중시켜 처리할 수 있다.
    - **카프카 클러스터의 복제 메커니즘은 다중 클러스터가 아닌 단일 클러스터에서만 동작하도록 설계되었다.**
    - 따라서 **다중 클러스터를 지원하기 위해** 아파치 카프카 프로젝트에는 **미러메이커**(MirrorMaker)라는 도구가 포함되어 있다.
    - 근본적으로 미러메이커도 카프카의 컨슈머와 프로듀서이며, 각 미러메이커는 큐(queue)로 상호 연결된다.
    - 그리고 하나의 카프카 클러스터에서 소비된 메시지를 다른 클러스터에서도 사용할 수 있도록 생성해준다.
10. **카프카를 사용하는 이유**
    1. **다중 프로듀서**
        - 여러 클라이언트가 많은 토픽을 사용하거나 같은 토픽을 같이 사용해도 카프카는 무리 없이 많은 프로듀서의 메시지를 처리할 수 있다.
        - 따라서 여러 프런트엔드 시스템으로부터 데이터를 수집하고 일관성을 유지하는 데 이상적이다.
        - 예를 들어, 여러 가지 마이크로서비스를 통해 사용자에게 콘텐츠를 서비스하는 사이트에서 모든 마이크로서비스가 같은 형식으로 작성한 페이지 뷰(page view)를 하나의 토픽으로 가질 수 있다.
        - 그 다음에 컨슈머 애플리케이션에서 그 사이트의 모든 애플리케이션 페이지 뷰를 하나의 스트림으로 받을 수 있다.
    2. **다중 컨슈머**
        - 다중 프로듀서와 더불어 카프카는 많은 컨슈머가 상호 간섭 없이 어떤 메시지 스트림도 읽을 수 있게 지원한다.
        - **한 클라이언트가 특정 메시지를 소비하면 다른 클라이언트에서 그 메시지를 사용할 수 없는 큐 시스템과는 다르다.**
        - 카프카 컨슈머는 컨슈머 그룹의 멤버가 되어 메시지 스트림을 공유할 수 있다.
    3. **디스크 기반의 보존**
        - 카프카는 다중 컨슈머를 처리할 수 있을 뿐만 아니라 **지속해서 메시지를 보존**할 수도 있다.
        - **따라서 컨슈머 애플리케이션이 항상 실시간으로 실행되지 않아도 된다.**
        - 메시지는 카프카 구성에 설정된 보존 옵션에 따라 디스크에 저장되어 보존된다. 그리고 보존 옵션을 토픽별로 선택할 수 있으므로, 컨슈머의 요구에 맞게 메시지 스트림마다 서로 다른 보존 옵션을 가질 수 있다.
        - **따라서 처리가 느리거나 접속 폭주로 인해 컨슈머가 메시지를 읽는 데 실패하더라도 데이터 유실될 위험이 없다.**
        - 또한 **보존 기간 동안 컨슈머가 동작하지 않더라도 메시지는 카프카에 보존되므로 프로듀서의 메시지 백업이 필요 없고, 컨슈머의 유지보수도 자유롭게 수행할 수 있다.**
        - 그리고 **컨슈머가 다시 실행되면 중단 시점의 메시지부터 처리할 수 있다.**
    4. **확장성**
        - 카프카는 확장성이 좋아서 어떤 크기의 데이터도 쉽게 처리할 수 있다.
        - 따라서 처음에는 실제로 잘 되는지 검증하는 의미에서 하나의 브로커로 시작한 후 세 개의 브로커로 구성된 소규모의 개발용 클러스터로 확장하면 좋다.
        - 그 다음에 데이터 증가에 따라 10개에서 심지어는 수백 개의 브로커를 갖는 대규모 클러스터로 업무용 환경을 구축하면 된다.
        - **확장 작업은 시스템 전체의 사용에 영향을 주지 않고 클러스터가 온라인 상태일 때도 수행될 수 있다.**
        - 여러 개의 브로커로 구성된 클러스터는 개별적인 브로커의 장애를 처리하면서 클라이언트에게 지속적인 서비스를 할 수 있기 때문이다.
        - 또한, 동시에 여러 브로커에 장애가 생겨도 정상적으로 처리할 수 있는 **복제 팩터**(replication factor)를 더 큰 값으로 구성할 수 있다.
    5. **고성능**
        - 프로듀서, 컨슈머, 브로커 모두 대용량의 메시지 스트림을 쉽게 처리할 수 있도록 확장될 수 있으며, 확장 작업도 1초 미만의 짧은 시간 내에 가능하다.
11. **데이터 생태계**
    - 데이터를 처리하기 위해 구축한 환경에는 많은 애플리케이션이 있다. 그리고 데이터를 생성하는 애플리케이션에 맞춰 입력 형식이 정의되며, 메프틱, 리포트 등으로 출력 형태가 정의된다. 또한, 특정 컴포넌트를 사용해서 시스템의 데이터를 읽은 후 다른 소스에서 받은 데이터를 사용해서 변환시킨다. 그 다음에 어디서든 사용될 수 있도록 최종 데이터를 데이터 기반 구조에 전달한다. 이런 작업은 고유한 콘텐츠와 크기, 용도를 갖는 다양한 유형의 데이터로 처리된다. 이것이 메시지 데이터의 처리 흐름이다.
    - 아파치 카프카는 **데이터 생태계의 순환 시스템을 제공**한다. 즉, **모든 클라이언트에 대해 일관된 인터페이스를 제공하면서 데이터 기반 구조의 다양한 멤버 간에 메시지를 전달**한다. 이때 메시지 구조를 정의한 스키마를 시스템에서 제공하면 프로듀서와 컨슈머가 어떤 형태로든 밀접하게 결합하거나 연결되지 않아도 되므로, 비즈니스 용도가 생기거나 없어질 때 관련 컴포넌트만 추가하거나 제거하면 된다. 또한, 프로듀서는 누가 자신의 데이터를 사용하는지, 그리고 컨슈머 애플리케이션이 몇 개 인지 신경쓰지 않아도 된다.
12. **이용 사례**
    1. **활동 추적**
        - 처음에 카프카는 사용자 활동 추적에 사용하기 위해 링크드인(LinkedIn)에서 설계되었다.
        - 즉, 사용자가 웹 사이트에 접속하여 액션(페이지 이동, 특정 콘텐츠 조회 등)을 수행하면 이것에 관한 메시지 데이터를 생성하는 프런트엔드 애플리케이션이 동작한다. 이런 추적 데이터는 해당 사이트의 페이지 뷰와 클릭 기록이 될 수 있으며, 그 사이트의 사용자가 자신의 프로파일에 추가하는 정보와 같이 더 복잡한 것이 될 수도 있다. 그리고 이와 같은 메시지는 하나 이상의 토픽으로 생성되어 카프카에 저장된 후 백엔드 애플리케이션에서 소비된다. 예를 들어, 리포트를 생성하거나, 머신 러닝 시스템에 제공되거나, 검색 결과를 변경하거나, 풍부한 사용자 경험을 제공하는 데 필요한 다른 작업을 수행하는 데 사용될 수 있다.
    2. **메시지 전송**
        - 카프카 또한 메시지를 전송하는데도 사용될 수 있으므로 사용자에게 알림 메시지를 전송해야 하는 애플리케이션에 유용하다.
        - 이때 각 애플리케이션에서 메시지 형식이나 전송 방법을 신경 쓰지 않게 메시지를 생성할 수 있다.
        - 전송될 모든 메시지를 하나의 애플리케이션이 읽어서 다음 작업을 일관성 있게 처리할 수 있기 때문이다.
            - 같은 룩앤필(look and feel)을 사용해서 메시지의 형식을 만든다.
            - 여러 개의 메시지를 하나의 알림 메시지로 전송하기 위해 합친다.
            - 사용자가 원하는 메시지 수신 방법을 적용한다.
        - 그리고 하나의 애플리케이션에서 처리하면 여러 애플리케이션에 중복된 기능을 가질 필요가 없을 뿐만 아니라 메시지 집중 처리와 같은 기능도 함께 수행할 수 있다.
    3. **메트릭과 로깅**
        - 카프카는 또한 애플리케이션과 시스템의 **메트릭과 로그 데이터**를 모으는 데 이상적이다.
        - 이것은 여러 애플리케이션에서 같은 타입의 메시지를 생성하는 대표적인 이용 사례다.
        - 이 경우 각 애플리케이션에서 장기적으로 메트릭을 생성하여 카프카 토픽으로 저장한 후 모니터링과 보안 시스템에서 그 데이터를 사용할 수 있다. 또한 하둡과 같은 오프라인 시스템에서 데이터 증가 예측과 같은 장기적인 분석을 위해 사용될 수도 있다. 로그 메시지는 모두 같은 방법으로 생성될 수 있으며, 엘라스틱서치(Elasticsearch)나 보안 분석 애플리케이션과 같은 로그 검색 전용 시스템으로 전달될 수 있다.
        - 이외에도 카프카를 사용하면 또 다른 장점이 있다. 즉, 특정 시스템을 변경해야 할 때(로그 저장 시스템을 변경할 시기가 되었을 때와 같이)도 프런트엔드 애플리케이션이나 메시지 수집 방법을 변경할 필요가 없다.
    4. **커밋 로그**
        - 카프카는 커밋 로그 개념을 기반으로 하므로 데이터베이스의 변경 사항이 카프카 메시지 스트림으로 생성될 수 있다.(커밋 로그는 데이터가 변경된 내역을 고르 메시지로 모아둔 것을 말한다).
        - 그리고 애플리케이션에서는 해당 스트림을 모니터링하여 발생 시점의 최신 변경 데이터를 받을 수 있다.
        - 이런 스트림을 변경 로그 스트림(changelog stream)이라고 하며, 이 스트림은 데이터베이스의 변경 데이터를 원격 시스템에 복제하는 데 사용되거나 여러 애플리케이션의 변경 데이터를 하나의 데이터베이스 뷰로 통합하는데 사용될 수 있다.
        - 이때 변경 로그 데이터를 모아두는 버퍼를 제공하기 위해 카프카의 메시지 보존 기능이 유용하게 사용된다.
        - 컨슈머 애플리케이션에 장애가 생긴 이후에도 해당 데이터를 다시 사용할 수 있기 때문이다.
        - 또한 더 오랫동안 데이터를 보존하고자 할 때는 키 하나당 하나의 변경 데이터만 보존하는 압축로그(log-compacted) 토픽을 사용할 수도 있다.
    5. **스트림 프로세싱**
        - 여러 종류의 애플리케이션에서 스트림 프로세싱(stream processing)을 지원한다. 카프카를 사용할 때도 대부분 스트림 프로세싱을 한다.
        - **일반적으로 이 용어는 하둡의 맵/리듀스(map/reduce)처리와 유사한 기능을 제공하는 애플리케이션에 사용된다.**
        - **단, 하둡은 긴 시간(어러 시간이나 여러 날)에 걸쳐 집중된 데이터를 사용하지만, 스트림 프로세싱에서는 메시지가 생성되자마자 실시간으로 데이터를 처리한다는 차이가 있다.**
        - 그리고 스트림 프로세싱을 하는 프레임워크에서는 작은 크기의 애플리케이션을 기능별로 여러 개 작성하여 카프카 메시지를 처리할 수 있다.
        - 예를 들어, 메트릭 데이터의 각종 수치를 계산하는 작업을 수행하는 애플리케이션, 다른 애플리케이션에서 효율적으로 메시지를 처리할 수 있게 파티션에 저장하는 애플리케이션, 여러 소스에서 받은 데이터를 사용해서 메시지를 변환하는 애플리케이션 등이다.

----------------------------------------------------

## Kafka installation and configuration
1. **AWS EC2에 카프카 설치**
    1. **AWS EC2 구축** (EC2 + Route 53 + Elastic beanstalk + RDS), pem파일 저장 [영상](https://www.youtube.com/playlist?list=PLSU3uVI3_Xyvpw2fIiUrKs8skZMwyr3LI) [![Youtube Badge](https://img.shields.io/badge/Youtube-ff0000?style=flat-square&logo=youtube&link=https://www.youtube.com/playlist?list=PLSU3uVI3_Xyvpw2fIiUrKs8skZMwyr3LI)](https://www.youtube.com/playlist?list=PLSU3uVI3_Xyvpw2fIiUrKs8skZMwyr3LI)
    2. pem 파일 위치에서 **AWS Server** 접속
        ```bash
        chmod 400 mykafka.pem
        
        ssh -i mykafka.pem ec20user@{aws ec2 public ip}
        ```
    3. Java, Kafka 설치 및 압축 풀기 / **Kafka 실행 최소 Heap Size 설정 제거**
        ```bash
        sudo yum install -y java-1.8.0-openjdk-devel.x86_64
        wget http://mirror.navercorp.com/apache/kafka/2.5.0/kafka_2.12-2.5.0.tgz
        tar -xvf kafka_2.12-2.5.0.tgz
        
        export KAFKA_HEAP_OPTS="-Xmx400m -Xms400m"
        ```
    4. **카프카 설정 파일 수정**
        ```bash
        vi config/server.properties
        
        listeners=PLAINTEXT://:9092
        advertised.listeners=PLAINTEXT://{aws ec2 public ip}:9092
        ```
    5. **주키퍼** 실행
        ```bash
        bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
        ```
    6. **카프카** 실행
        ```bash
        bin/kafka-server-start.sh -daemon config/server.properties
        ```
    7. 로그 확인
        ```bash
        tail -f logs/*
        ```
2. **Local**에 카프카 설치 및 테스트
    1. 카프카 설치 및 압축 풀기
        ```bash
        curl http://mirror.navercorp.com/apache/kafka/2.5.0/kafka_2.13-2.5.0.tgz
        tar -xvf kafka_2.13-2.5.0.tgz
        ```
    2. 테스트
        ```bash
        cd kafka_2.13-2.5.0/bin
        
        - 토픽 생성
        ./kafka-topics.sh --create --bootstrap-server {aws ec2 public ip}:9092 --replication-factor 1 --partitions 3 --topic test
        
        - Producer 실행
        ./kafka-console-producer.sh --bootstrap-server {aws ec2 public ip}:9092 --topic test
        
        - Consumer 실행
        ./kafka-console-consumer.sh --bootstrap-server {aws ec2 public ip}:9092 --topic test --from-beginning
        ./kafka-console-consumer.sh --bootstrap-server {aws ec2 public ip}:9092 --topic test -group testgroup --from-beginning
        
        - Consumer Group List 확인
        ./kafka-consumer-groups.sh --bootstrap-server {aws ec2 public ip}:9092 --list
        
        - Consumer Group 상태 확인
        ./kafka-consumer-groups.sh --bootstrap-server {aws ec2 public ip}:9092 --group testgroup --describe
        
        - 가장 낮은 Offset으로 Reset
        ./kafka-consumer-groups.sh --bootstrap-server {aws ec2 public ip}:9092 --group testgroup --topic test --reset-offsets --to-earliest --execute
        
        - 특정 파티션을 특정 Offset으로 Reset
        ./kafka-consumer-groups.sh --bootstrap-server {aws ec2 public ip}:9092 --group testgroup --topic test:1 --reset-offsets --to-offset 10 --execute
        ```
        
3. **구성 옵션 설정 관련**
    1. **주키퍼** 구성 옵션 ((KAFKA_HOME)/config/zookeeper.properties)
        - **tickTime** : initLimit 설정 관여
        - **dataDir** : 각 서버는 dataDir에 지정된 디텍터리에 myid라는 이름의 파일을 갖고 있어야 한다. 그리고 이 파일은 구성 파일에 지정된 것과 일치되는 각 서버의 ID 번호를 포함해야 한다.
        - **clientPort** : 주키퍼에 접속하는 클라이언트는 clientPort에 지정된 포트 번호로 앙상블과 연결 (앙상블의 서버들은 세 가지 포트 모두를 사용해서 상호 통신)
        - **initLimit** : 팔로어(대기 서버)가 리더(데이터 쓰기를 하는 서버)에 접속할 수 있는 시간 (initLimit이 20, tickTime이 2000 일 경우 20*2000 ms = 40초)
        - **syncLimit** : 리더가 될 수 있는 팔로어의 최대 개수
        - **앙상블의 서버** 내역 설정
            - **server.X=hostname:peerPort:leaderPort**
            - X : 각 서버의 ID 번호이며 정수
            - hostname : 각 서버의 호스트 이름이나 IP 주소
            - peerPort : 앙상블의 서버들이 상호 통신하는 데 사용하는 TCP 포트 번호
            - leaderPort : 리더를 선출하는 데 사용하는 TCP 포트 번호
    2. **브로커** 구성 옵션 ((KAFKA_HOME)/config/server.properties)
        - **broker.id** : 모든 카프카 브로커는 broker.id에 설정하는 정수로 된 번호를 가져야 한다. 이것의 기본 값은 0 이지만 어떤 값도 가능하다. **단 하나의 카프카 클러스터 내에서 고유한 값이어야 한다.**
        - **port** : 카프카의 실행에 사용되는 TCP 포트, **기본 구성값은 9092**, 이 값은 어떤 포트 번호로도 설정할 수 있지만 1024보다 작은 값으로 설정한다면 카프카가 root 권한으로 시작되어야 한다.
        - **zookeeper.connect** : **브로커의 메타데이터를 저장하기 위해 사용되는 주키퍼의 위치**, 기본 구성 값은 localhost:2181 (로컬 호스트의 2181 포트에서 실행되는 주키퍼를 사용), "호스트이름:포트/경로" 형식으로 지정
        - **log.dirs** : **카프카는 모든 메시지를 로그 세그먼트 파일에 모아서 디스크에 저장**한다. 그리고 이 파일은 **log.dirs에 지정된 디렉터리에 저장**된다. 여러 경로를 쉼표로 구분하여 지정할 수 있고, 두 개 이상의 경로가 지정되면 해당 브로커가 모든 경로에 파티션을 저장한다. (**주의, 브로커가 새로운 파티션을 저장할 때는 가장 적은 디스크 용량을 사용하는 경로가 아닌 가장 적은 수의 파티션을 저장한 경로를 사용한다.**)
        - **num.recovery.threads.per.data.dir** : 카프카는 구성 가능한 스레드 풀을 사용해서 로그 세그먼트를 처리한다. 기본적으로는 로그 디렉터리당 하나의 스레드만 사용한다. 이 스레드들은 브로커의 시작과 종료 시에만 사용되므로 병행 처리를 하도록 많은 수의 스레드를 설정하는 것이 좋다. 그리고 **이 매개변수에 설정하는 스레드의 수는 log.dirs에 지정된 로그 디렉터리마다 적용된다는 것에 유의**해야 한다. (log.dirs에 지정된 경로가 3개, num.recovery.threads.per.data.dir을 8로 설정하면 전체 스레드의 개수는 24개)
        - **auto.create.topics.enable** : 기본 설정에는 true로 지정되어 있다. 직접 토픽 생성을 관리 할 때에는 auto.create.topics.enable 값을 false로 지정하면 된다.
        - **num.partitions** : 새로운 토픽이 몇 개의 파티션으로 생성되는지를 나타내며, 주로 자동 토픽 생성이 활성화될 때 사용된다. (**주의, 토픽의 파티션 개수는 증가만 하고 감소될 수 없다.**)
        - **log.retention.ms** : **얼마 동안 메시지를 보존할지** 설정. 시간 단위인 **log.retention.hours** 를 주로 사용 (기본 값은 1주일인 168시간), **log.retention.minutes**와 **log.retention.ms** 를 사용할 수 있다. 만약 두 개 이상이 같이 지정되면 더 작은 시간 단위의 매개변수 값이 사용되므로 항상 log.retention.ms의 값이 사용된다.
        - **log.retention.bytes** : **저장된 메시지들의 전체 크기를 기준으로 만기를 처리하는 방법**, 이 값은 모든 파티션에 적용된다. (하나의 토픽이 8개의 파티션으로 되어 있고 log.retention.bytes의 값이 1GB로 설정되면 해당 토픽에 저장되는 메시지들의 전체 크기는 최대 8GB가 된다.) 이러한 **메시지 보존은 토픽이 아닌 각 파티션별로 처리된다.**
        - **log.segment.bytes** : 위의 **메시지 보존 설정은 개별적인 메시지가 아닌 로그 세그먼트 파일을 대상으로 처리**된다. 메시지가 카프카 브로커에 생성될 때는 해당 파티션의 로그 세그먼트 파일 끝에 추가된다. 이때 log.segment.bytes 매개변수에 지정된 크기가 되면 해당 로그 세그먼트 파일이 닫히고 새로운 것이 생성되어 열린다. 만약 토픽의 데이터 저장률이 낮다면 로그 세그먼트의 크기를 조정하는 것이 중요하다.(**하루에 100MB의 메시지만 토픽에 수록되고 log.segment.bytes의 값이 1GB로 설정되어 있다면, 하나의 로그 세그먼트를 채우는 데 10일이 소요될 것이다. 그러나 로그 세그먼트 파일은 닫혀야만 만기가 될 수 있으므로 만일 log.retention.ms가 1주일로 설정되어 있다면 로그 세그먼트에 저장된 메시지들은 17일 동안 보존될 것이다. 왜냐하면 마지막 메시지를 쓰면서 로그 세그먼트 파일이 닫히는 데 10일이 소요되고, 다시 보존 시간이 7일이 지나야 만기가 되기 때문**)
        - **log.segment.ms** : **시간을 지정해서 로그 세그먼트 파일이 닫히는 것을 제어**할 수 있다. (log.retention.bytes와 log.retention.ms 매개변수처럼 log.segment.bytes와 log.segment.ms는 상호 연관된다.)
        - **message.max.bytes** : 카프카 브로커는 **쓰려는 메시지의 최대 크기를 제한**할 수 있다. 이때는 기본값이 1000000 (1MB)인 message.max.bytes 매개변수를 지정하면 된다. 그러면 이 값보다 큰 메시지를 전송하려는 프로듀서에게는 브로커가 에러를 보내고 메시지를 받지 않는다. 브로커에 지정되는 다른 모든 바이트 크기와 마찬가지로 이 값도 압축된 메시지의 크기를 나타낸다.
        
----------------------------------------------------

## Kafka Producer
1. **필수 구성 옵션**
    1. **boostrap.servers**
        - **카프카 클러스터에 연결하기 위한 브로커 목록**
        - 프로듀서가 사용하는 브로커들의 host:port 목록을 설정
        - 연결되면 프로듀서가 더 많은 정보를 얻을 수 있기 때문에 **모든 브로커를 포함할 필요는 없다. 그러나 최소한 두 개 이상을 포함하는 것이 좋다. 한 브로커가 중단되는 경우에도 프로듀서는 여전히 클러스터에 연결될 수 있기 때문이다.**
    2. **key.serializer**
        - **메시지 키 직렬화에 사용되는 클래스**
        - 프로듀서가 생성하는 레코드(ProducerRecord 객체)의 메시지 키를 직렬화하기 위해 사용되는 클래스 이름을 이 속성에 설정
        - 카프카 브로커는 바이트 배열로 메시지를 받는다. 그러나 카프카의 프로듀서 인터페이스에서는 매개변수화 타입을 사용해서 키와 값의 쌍으로 된 어떤 자바 객체도 전송할 수 있으므로 알기 쉬운 코드를 작성할 수 있다. 단, 이때는 그런 객체를 바이트 배열로 변환하는 방법을 프로듀서가 알고 있어야 한다.
        - key.serializer에 설정하는 클래스는 org.apache.kafka.common.serialization.Serializer 인터페이스를 구현해야 한다. 프로듀서는 이 클래스를 사용해서 키 객체를 바이트 배열로 직렬화한다.
        - 카프카 클라이언트 패키지에는 세 가지 타입의 직렬화를 지원하는 ByteArraySerializer, StringSerializer, IntegerSerializer가 포함되어 있다. 이런 타입들을 직렬화할 때는 직렬처리기를 따로 구현하지 않아도 된다.
        - **레코드의 키는 생략하고 값만 전송하고자 할 때도 설정해야 한다.**
    3. **value.serializer**
        - **메시지 값을 직렬화하는데 사용되는 클래스**
        - 레코드의 메시지 값을 직렬화하는 데 사용하는 클래스 이름을 설정
        - 직렬화하는 방법은 key.serializer와 동일하다.
    4. **기본 설정**
        ```java
        // Properties 객체 생성
        private Properties kafkaProps = new Properties();
        
        // 속성과 값 설정
        kafkaProps.put("bootstrap.servers", "broker1:9092, broker2:9092");
        kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        
        // 새로운 프로듀서 객체 생성, Properties 객체를 생성자 인자로 전달한다.
        Producer<String, String> producer = new KafkaProducer<String, String>(kafkaProps);
        ```
2. **선택 구성 옵션** (Default 값 존재)
    1. **acks**
        - 레코드 전송 **신뢰도 조절** (Replica)
        - 전송된 레코드를 수신하는 파티션 리클리카의 수를 제어, 메시지가 유실될 가능성에 영향을 준다.
        - **acks=0** : **프로듀서는 브로커의 응답을 기다리지 않는다.** 메시지 유실 가능성이 큰데 반해, 매우 높은 처리량이 필요할 때 사용
        - **acks=1** : **리더 리플리카가 메시지를 받는 순간 프로듀서는 브로커로부터 성공적으로 수신했다는 응답을 받는다.** 그러나 만일 리더에서 메시지를 쓸 수 없다면 프로듀서는 에러 응답을 받을 것이고 데이터 유실을 막기 위해 메시지를 다시 전송할 수 있다. 또한 리더가 중단되고 해당 메시지를 아직 복제하지 않은 리플리카가 새로운 리더로 선출될 경우에도 여전히 메시지가 유실될 수 있다. 이때는 동기식이나 비동기식 중 어떤 방법으로 메시지를 전송했는가에 따라 처리량이 달라질 수 있다.
        - **acks=all** : **동기화된 모든 리플리카가 메시지를 받으면 프로듀서가 브로커의 성공 응답을 받는다.** 하나 이상의 브로커가 해당 메시지를 갖고 있으므로, 파티션이나 리더 리플리카에 문제가 생기더라도 메시지가 유실되지 않기 때문에 가장 안전한 형태이다. 그러나 대기 시간이 길어진다.
    2. **compression.type**
        - **snappy, gzip, lz4 중 하나로 압축**하여 전송
        - 기본적으로 메시지는 압축되지 않은 상태로 전송되지만, 이 매개변수를 설정하면 압축되어 전송된다. 따라서 **네트워크 처리량이 제한적일 때** 사용하면 좋다.
        - 메시지 압축을 사용함으로써 카프카로 메시지를 전송할 때 병목 현상이 생길 수 있는 네트워크와 스토리지 사용을 줄일 수 있다.
    3. **retries**
        - 클러스터 장애에 대응하여 **메시지 전송을 재시도하는 횟수**
        - 프로듀서가 서버로부터 받는 에러가 일시적일 수 있기 때문에 메시지 전송을 포기하고 에러로 처리하기 전에 프로듀서가 메시지를 재전송하는 횟수를 제어할 수 있다.
        - 각 재전송 간에 **retry.backoff.ms**에 설정한 시간동안 대기하고 전송한다.
    4. **buffer.memory**
        - **브로커에 전송될 메시지의 버퍼로 사용될 메모리 양**
        - 브로커들에게 전송될 메시지의 버퍼로 사용할 메모리의 양을 설정
        - 메시지들이 서버에 전달될 수 있는 것보다 더 빠른 속도로 애플리케이션에서 전송된다면 추가로 호출되는 send() 메서드는 block.on.buffer.full(max.block.ms) 매개변수의 설정에 따라 예외를 일시 중단 시키거나 바로 발생시킨다.
    5. **batch.size**
        - **여러 데이터를 함께 보내기 위한 레코드 크기**
        - 같은 파티션에 쓰는 다수의 레코드가 전송될 때는 프로듀서가 그것들을 배치로 모은다. 이 매개변수는 각 배치에 사용될 메모리양(byte)을 제어한다. 그리고 해당 배치가 가득 차면 그것의 모든 메시지가 전송된다.
        - **하지만 배치가 가득 찰 때까지 프로듀서가 기다리는 것은 아니다.** 프로듀서는 절반만 채워진 배치와 심지어는 하나의 메시지만 있는 배치도 전송한다.
        - 크게 설정할 경우 메시지 전송은 지연되지 않지만 메모리를 더 많이 사용하게 된다. 반면에 작게 설정하면 프로듀서가 너무 자주 메시지를 전소앻야 하므로 부담을 초래한다.
    6. **linger.ms**
        - **현재의 배치를 전송하기 전까지 기다리는 시간**
        - **현재의 배치가 가득 찼거나, linger.ms에 설정된 제한 시간이 되면 프로듀서가 메시지 배치를 전송**한다.
        - 대기 시간이 증가하는 단점은 있지만, 동시에 처리량도 증가하는 장점이 있다.
    7. **client.id**
        - **어떤 클라이언트인지 구분하는 식별자**
        - 어떤 문자열도 가능하며, 어떤 클라이언트에서 전송된 메시지인지 식별하기 위해 브로커가 사용한다.
        - **주로 로그 메시지와 메트릭 데이터의 전송에 사용**
    8. **max.in.flight.requests.per.connection**
        - **서버의 응답을 받지 않고 프로듀서가 전송하는 메시지의 개수를 제어**한다.
    9. **timeout.ms, request.timeout.ms, metadata.fetch.timeout.ms**
        - timeout.ms : 동기화된 리플리카들이 메시지를 인지하는 동안 브로커가 대기하는 시간 제어
        - request.timeout.ms : 데이터를 전송할 때 프로듀서가 서버 응답을 기다리는 제한 시간
        - metadata.fetch.timeout.ms : 메타데이터를 요청할 때 프로듀서가 서버 응답을 기다리는 제한 시간
    10. **max.block.ms**
        - send() 메서드를 호출할 때 프로듀서의 전송 버퍼가 가득 차거나 partitionsFor() 메소드로 메타데이터를 요청했지만 사용할 수 없을 때 프로듀서가 max.block.ms의 시간동안 일시 중단된다 (그 사이에 에러가 해결될 수 있도록). 그 다음에 max.block.ms의 시간이 되면 시간 경과 예외가 발생한다.
    11. **max.request.size**
        - **프로듀서가 전송하는 쓰기 요청의 크기를 제어**한다.
        - 전송될 수 있는 가장 큰 메시지의 크기와 프로듀서가 하나의 요청으로 전송할 수 있는 메시지의 최대 개수 모두를 이 매개변수로 제한한다.
        - **브로커 설정의 message.max.bytes와 이 값을 일치되도록 설정하는 것이 좋다. 그래야만 브로커가 거부하는 크기의 메시지를 프로듀서가 전송하지 않을 것이기 때문이다.**
    12. **receive.buffer.bytes, send.buffer.bytes**
        - 데이터를 읽고 쓸 때 소켓이 사용하는 TCP 송수신 버퍼의 크기를 나타낸다. 만일 이 매개변수들이 -1로 설정되면 운영체제의 기본값이 사용된다.
3. **메시지 전송 방법**
    1. **Fire-and-forget (전송 후 망각)**
        - 가장 간단한 방법, send() 메서드로 메시지를 **전송만 하고 성공 또는 실패 여부에 따른 후속 조치를 취하지 않는 방법**, 카프카는 가용성이 높고 전송에 실패할 경우에 프로듀서가 자동으로 재전송을 시도하므로 대부분의 경우에 성공적으로 메시지가 전송된다. 그러나 일부 메시지가 유실될 수도 있다.
        ```java
        // ("topic", "key", "value")
        ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        
        try {
            producer.send(record);
        } catch(Exception e){
            e.printStackTrace();
        }
        ```
    2. **Synchronous send (동기식 전송)**
        - send() 메서드로 메시지를 전송하면 자바의 Future 객체가 반환된다. 그 다음에 **Future 객체의 get() 메서드를 곧바로 호출하면 작업이 완료될 때까지 기다렸다가 브로커로부터 처리 결과가 반환**되므로 send()가 성공적으로 수행되었는지 알 수 있다.
        ```java
        ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        
        try {
            producer.send(record).get();
        } catch(Exception e){
            e.printStackTrace();
        }
        ```
    3. **Asynchronous send (비동기식 전송)**
        - send() 메서드를 호출할 때 **콜백 메서드**를 구현한 객체를 매개변수로 전달한다. 이 객체에 구현된 콜백 메서드는 카프카 브로커로부터 응답을 받을 때 자동으로 호출되므로 send()가 성공적으로 수행되었는지 알 수 있다.
        ```java
        private class DemoProducerCallback implements Callback{
            @Override
            public void onCompletion(RecordMetadata recordMetadata, Exception e){
                if(e != null){
                    e.printStackTrace();
                }
            }
        }
        
        ProducerRecord<String, String> record = new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
        
        producer.send(record, new DemoProducerCallback());
        ```
4. **직렬처리기** (**Detailed** : can be found in each directory.)
    1. **커스텀 직렬 처리기** (Customer.java, CustomerSerializer.java)
        - 예를 들어, 고객을 나타내는 간단한 클래스는 customerID와 customerName 속성을 갖는다.
        - 기존의 int형의 ID를 Long 타입으로 변경해야 하거나 새로운 필드를 추가한다면, **기존 메시지와 새로운 메시지 간의 호환성을 유지하는 데 심각한 문제**가 생길 것이다.
        - 이런 이유로 JSON, 아파치 Avro, Thrift, Protobuf 같은 **범용 직렬처리기와 역직렬처기리의 사용을 권장**한다.
    2. **아파치 Avro를 사용해서 직렬화하기**
        - Avro는 언어 중립적인 데이터 직렬화 시스템이다.
        - Avro는 주로 JSON(JavaScript Object Notation) 형식으로 기술하며, 직렬화 역시 JSON을 지원하지만 주로 이진 파일을 사용한다.
        - **Avro가 파일을 읽고 쓸 때는 스키마가 있다고 간주한다** (Avro 파일에는 스키마가 포함된다).
        - 메시지를 쓰는 애플리케이션이 새로운 스키마(데이터 구조)로 전환하더라도 **해당 메시지를 읽는 애플리케이션은 일체의 변형 없이 계속해서 메시지를 처리할 수 있다는 것이 가장 큰 장점**이다.
        - **조건**
            1. **데이터를 쓰는 데 사용되는 스키마와 읽는 애플리케이션에서 기대하는 스키마가 호환**될 수 있어야 한다.
            2. 직렬화된 데이터를 원래의 형식으로 변환하는 **역직렬처리기는 데이터를 쓸 때 사용되었던 스키마를 사용해야 한다**. 그 스키마가 해당 데이터를 읽는 애플리케이션이 기대하는 것과 다른 경우에도 마찬가지다. 따라서 Avro 파일에는 데이터를 쓸 때 사용되었던 스키마가 포함된다. 그러나 카프카 메시지를 Avro로 직렬화할 때는 Avro 파일을 사용하는 것보다 **스키마 레지스트리**를 사용하는 더 좋은 방법이 있다.
    3. **Avro 레코드 사용하기** (AvroSerializer.java, GenericAvroSerializer.java)
        1. 데이터 파일에 스키마 전체를 저장하는 Avro 파일과는 다르게, 데이터를 갖는 각 레코드에 스키마 전체를 저장한다면 레코드 크기가 2배 이상이 되므로 큰 부담이 될 것이다. 그러나 레코드를 **읽을 때 Avro는 스키마 전체를 필요로 하므로 레코드 크기의 부담이 없게 하려면 스키마를 레코드가 아닌 다른 곳에 두어야 한다.** 이렇게 하기 위해 아파치 카프카에서는 아키텍처 패턴에 나온대로 **스키마 레지스트리를 사용**한다. 스키마 레지스트리는 아파치 카프카에 포함되지 않았지만 **오픈 소스에서 선택**할 수 있는 것들이 있다. 여기서는 [Confluent](https://github.com/confluentinc/schema-registry) 스키마 레지스트리를 사용. 
        2. **카프카에 데이터를 쓰는 데 사용되는 모든 스키마를 레지스트리에 저장하는 것이 스키마 레지스트리의 개념**이다. 그 다음에 **카프카에 쓰는 레코드에는 사용된 스키마의 식별자만 저장하면 된다.** 그러면 컨슈머는 해당 식별자를 사용해서 스키마 레지스트리의 스키마를 가져온 후 이 스키마에 맞춰 데이터를 역직렬화할 수 있다. 이때 모든 작업이 직렬처리기와 역직렬처리기에서 수행된다는 것이 중요하다. 즉, 카프카에 데이터를 쓰는 코드에서는 스키마 레지스트리 관련 코드를 추가하지 않고 종전 직렬처리기를 사용하듯이 Avro를 사용하면 된다.
        3. **필요 jar 및 Gradle Dependency 추가** (Producer_Serializer/lib Directory)
            1. [Apache Avro](https://mvnrepository.com/artifact/org.apache.avro/avro/1.8.2)
            2. [Confluent Avro Serializer](http://packages.confluent.io/maven/io/confluent/kafka-avro-serializer/)
                - Influent는 Gradle Dependency Error가 잘 발생하여 직접 jar 파일을 다운로드해서 사용  
5. **파티션** (**Detailed** : can be found in each directory.)
    1. **Key 사용 목적**
        1. **메시지를 식별**하는 추가 정보
        2. 메시지를 쓰는 **토픽의 여러 파티션 중 하나를 결정**하기 위해서 (같은 키를 갖는 모든 메시지는 같은 파티션에 저장된다.)
    2. **ProducerRecord** 객체 생성
        1. **키와 값이 쌍**으로 된 레코드
            ```java
            ProducerRecord<String, String> record = new ProducerRecord<>("topic","key","value");
            ```
        2. **키가 없는 레코드** (이 경우 key = null)
            ```java
            ProducerRecord<String, String> record = new ProducerRecord<>("topic", "value");
            ```
    3. **키가 null 이면서 카프카의 기본 파티셔너**를 사용
        - 사용 가능한 토픽의 파티션들 중 하나가 **무작위로 선택**되어 해당 레코드 저장
        - 각 파티션에 저장되는 메시지 개수의 균형을 맞추기 위해 **라운드 로빈** 알고리즘 사용
    4. **키가 있으면서 기본 파티셔너** 사용
        - 카프카에서 **키의 해시 값**을 구한 후 그 값에 따라 **특정 파티션**에 메시지를 저장
        - 파티셔너 자신의 해싱 알고리즘을 사용하므로 자바 버전이 업그레이드 되어도 해시 값은 변하지 않는다.
        - 같은 키는 항상 같은 파티션에 대응되는 것이 중요하므로 토픽의 모든 파티션들을 대상으로 대응 되는 파티션을 선택한다.
        - 그러나 **토픽에 새로운 파티션들을 추가하는 순간 일관성이 보장되지 않는다.**
        - 같은 키와 대응되는 파티션이 변경되지 않게 하려면 충분한 수의 파티션을 갖는 토픽을 생성하고 이후로는 파티션을 추가하지 않는 것이 가장 쉬운 방법
    5. **커스텀 파티셔너**
        - 반드시 키의 해시 값을 사용해서 대응되는 파티션을 찾아야 하는 것은 아니다.
        - 해시 값을 사용하는 기본 파티셔너를 사용한다면, **하나의 파티션이 나머지 파티션보다 더 많은 메시지를 갖게 되는 경우**가 생긴다. 따라서 서버의 **스토리지 공간 부족**으로 처리 속도가 저하되는 등의 문제가 발생한다.
        - 이런 경우 커스텀 파티셔너를 사용
        
----------------------------------------------------

## Kafka Consumer
1. **중요 개념**
    - 카프카 컨슈머들은 **컨슈머 그룹**(Consumer Group)에 속한다.
    - 다수의 컨슈머가 같은 토픽을 소비하면서 같은 컨슈머 그룹에 속할 때는 **각 컨슈머가 해당 토픽의 서로 다른 파티션을 분담해서 메시지를 읽을 수 있다.**
    - 하나의 컨슈머 그룹에 더 많은 컨슈머를 추가하면 카프카 토픽의 데이터 소비를 확장할 수 있다.
    - 그러나 **한 토픽의 파티션 개수보다 더 많은 수의 컨슈머를 추가하는 것은 의미가 없다**.
    - **각 컨슈머는 스레드로 구현되며 병행으로 실행된다.**
    - **같은 토픽의 데이터를 다수의 애플리케이션이 읽어야 할 때는 각 애플리케이션이 자신의 컨슈머 그룹을 갖도록 해야 한다.** 각 애플리케이션에서 하나 이상의 토픽에 저장된 모든 메시지들을 읽어야 할 때는 애플리케이션마다 컨슈머 그룹을 생성한다. 그리고 토픽의 메시지 소비를 확장할 때는 기존 컨슈머 그룹에 새로운 컨슈머를 추가한다.
2. 컨슈머 그룹과 **리밸런싱** (Re-Balancing)
    - **컨슈머 그룹의 컨슈머들은 자신들이 읽는 토픽 파티션의 소유권을 공유**한다.
    - **새로운 컨슈머를 그룹에 추가하면 이전에 다른 컨슈머가 읽던 파티션의 메시지들을 읽는다.** 특정 컨슈머가 **문제가 생겨 중단될 때도** 마찬가지다. 즉, **그 컨슈머가 읽던 파티션은 남은 컨슈머 중 하나가 재할당받아 읽는다.**
    - 해당 컨슈머 그룹이 읽는 토픽들에 변경사항이 생길 때(파티션 추가 등)에도 **파티션의 재할당**이 생길 수 있다.
    - **한 컨슈머로부터 다른 컨슈머로 파티션 소유권을 이전하는 것을 리밸런싱이라고 한다.** 리밸런싱은 컨슈머 그룹의 가용성과 확장성을 높여주므로 중요하다.
    - 그러나 정상적인 처리에서는 그다지 바람직하지 않다. **리밸런싱을 하는 동안 컨슈머들은 메시지를 읽을 수 없으므로 해당 컨슈머 그룹 전체가 잠시나마 사용 불가능 상태**가 되기 때문이다.
    - 또한 한 컨슈머로부터 다른 컨슈머로 파티션이 이전될 때는 해당 컨슈머의 이전 파티션에 관한 현재 상태 정보가 없어진다. 따라서 캐시 매모리에 있던 데이터도 지워지므로 컨슈머의 해당 파티션 상태 정보가 다시 설정될 때까지 애플리케이션의 실행이 느려질 수 있다.
    - **그룹 조정자로 지정된 카프카 브로커에게 컨슈머가 하트비트를 전송하면 자신이 속한 컨슈머 그룹의 맴버십과 자신에게 할당된 파티션 소유권을 유지할 수 있다.**
    - **그룹조정자는 GroupCoordinator 클래스의 인스턴스로 생성되어 백그라운드 프로세스로 실행되는 카프카 브로커**다.
    - **하트비트는 컨슈머의 상태를 알리기 위해 전송되는 신호이며, 컨슈머가 일정 시간 간격으로 하트비트를 전송한다면, 그 컨슈머는 살아있고 잘 동작하며 자신의 파티션 메시지를 처리 가능한 것으로 간주한다.**
    - **하트비트는 컨슈머가 폴링할 때 또는 읽은 메시지를 커밋할 때 자동으로 전송**된다.
    - 만일 컨슈머가 세션 타임아웃 시간이 경과될때까지 하트비트 전송을 중단하면 GroupCoordinator가 해당 컨슈머를 중단된것으로 간주하고 리밸런싱을 시작한다. 또는 컨슈머에 문제가 생겨 중단되어도 마찬가지로 처리된다.
    - 그리고 그동안은 중단된 컨슈머가 소유한 파티션의 메시지가 처리되지 않는다.
    - 컨슈머가 정상적으로 종료될때는 그룹조정자에게 떠난다는 것을 알려주면 되며, 이때 그룹조정자는 처리 공백을 줄이기 위해 곧바로 리밸런싱을 시작시킨다. 
    - 파티션 할당 절차
        - 컨슈머가 그룹에 합류하고 싶을때는 그룹 조정자에게 JoinGroup 요청을 전송하면 된다. 이때 **그룹에 첫번째로 합류하는 컨슈머는 그룹 리더**가 된다. **리더는 그룹조정자로부터 해당 그룹의 모든 컨슈머 내역을 받을 수 있으며, 그 그룹의 각 컨슈머들에게 파티션을 할당하는 책임을 갖는다**. 그리고 이때 PartitionAssignor 클래스를 사용해서 어떤 파티션들을 어느 컨슈머가 처리할 것인지 결정한다.
3. **필수 구성 옵션**
    1. **bootstrap.servers** : KafkaProducer와 같은 방법
    2. **key.deserializer, value.deserializer** : 프로듀서에 정의되는 직렬처리기와 반대 기능을 수행하는 **역직렬처리기**
        - 자바 객체를 바이트 배열로 변환하는 직렬화 대신 직렬화된 바이트 배열의 값을 다시 자바 객체로 환원하는 역직렬화 클래스 지정
4. **카프카 컨슈머 생성**
    - 컨슈머 클래스인 **KafkaConsumer** 인스턴스 생성
        - **Properties 인스턴스를 생성한 후 이것을 KafkaConsumer 생성자의 인자로 전달**한다.
    ```java
    Properties props = new Properties();
    
    props.put("bootstrap.servers", "broker1:9092, broker2:9092");
    // 여기서 생성되는 컨슈머가 속한 컨슈머 그룹의 이름
    props.put("group.id", "TestG");
    props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    
    KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);
    ```
5. **토픽 구독하기** (Subscribe)
    1. 컨슈머를 생성한 다음에는 하나 이상의 토픽을 구독해야 한다. 이때 **subscribe**() 메서드를 사용하며 이 메서드는 **토픽 이름을 저장한 List를 매개변수로 받는다.**
    2. 우리가 읽고자 하는 모든 토픽 이름들을 **한번에 전달해야 한다. 나중에 추가할 수 없다.**
        ```java
        // 토픽이름:cusomerCountries
        // 정규 표현식을 매개변수로 전달하여 호출할 수도 있다. 새로운 토픽을 생헝하면 그 즉시 리밸런싱이 수행된 후 읽기 시작
        // consumer.subscribe(Pattern.compile("test.*"));
        consumer.subscribe(Collections.singletonList("customerCountries"));
        ```
    3. **폴링 루프**
        - **컨슈머 API의 핵심은 서버로부터 연속적으로 많은 데이터를 읽기 위해 폴링하는 루프에 있다.**
        - 컨슈머의 토픽 구독 요청이 정상적으로 처리되면 그 다음에 폴링 루프에서 데이터를 읽는 데 필요한 모든 상세 작업을 처리한다.
        - **poll** : **가장 중요한 메소드, 컨슈머는 카프카를 계속 폴링해야 한다. 그렇게 하지 않으면 죽은 것으로 간주되어 해당 컨슈머가 읽는 파티션들을 같은 그룹의 다른 컨슈머가 처리하기 때문이다.**
        - **새로운 컨슈머에서 최초로 poll()을 호출할 때는 이 메시드에서 그룹조정자를 찾고, 컨슈머 그룹에 추가시키며, 해당 컨슈머에게 할당된 파티션 내역을 받는다. 하트비트 전송도 poll() 메서드에서 자동으로 수행**된다.
        ```java
        try{
            while(true){
                // 100 : 타임아웃 간격 ms
                ConsumerRecords<String, String> records = consumer.poll(100);
                
                for(ConsumerRecord<String, String> record : records){
                    // 데이터 상세 처리
                    // record.topic(), record.partition(), record.offset()
                    // record.key(), record.value()
                }
            }
        } finally{
            consumer.close();
        }
        ```

6. **선택 구성 옵션** (Default 값 존재)
    1. **fetch.min.bytes**
        - 레코드들을 가져올 때 **브로커로부터 받기 원하는 데이터의 최소량** 지정
        - 브로커가 컨슈머로부터 레코드 요청을 받았찌만 읽을 레코드의 양이 이 값보다 작다면, 브로커는 더 많은 메시지가 모일 때까지 기다렸다가 전송
        - **컨슈머와 브로커 모두의 작업량을 줄여준다.**
    2. **fetch.max.wait.ms**
        - **설정 시간만큼 기다렸다가 전송** (fetch.min.bytes와 연관), 기본 500ms
    3. **max.partition.fetch.bytes**
        - **서버가 파티션당 반환하는 최대 바이트 수를 제어**, 기본값 1MB
        - 이 값은 브로커가 허용하는 가장 큰 메시지 크기(**브로커 구성, max.message.size**)보다 더 큰 값이어야 한다.
        - 컨슈머가 데이터를 처리하는 데 걸리는 시간을 고려해야 한다.
        - 세션 타임아웃과 이에 따른 리밸런싱을 방지하는 데 충분한 시간 간격으로 컨슈머가 poll() 메소드를 호출해야 한다. 만일 한번의 poll() 호출에서 반환하는 데이터 양이 매우 크다면 컨슈머가 그것을 처리하는 시간이 오래 걸릴수 있다. 따라서 세션 타임아웃을 방지하는 시간에 맞춰 폴링 루프의 다음 반복을 실행하지 못하게 될것이다. 만일 이런일이 생긴다면 이 값을 더 작은 값으로 설정하거나 세션 타임아웃 시간을 늘리면 된다.
    4. **session.timeout.ms**
        - **컨슈머가 브로커와 연결이 끊기는 시간**, 기본 10초
        - 만일 컨슈머가 그룹조정자에게 하트비트를 전송하지 않으면서 이 값으로 설정한 시간이 경과되면 이 컨슈머는 실행종료된것으로 간주되고 그룹조정자는 그 컨슈머의 파티션들을 해당 그룹의 다른 컨슈머에게 할당하기 위해 해당 컨슈머 그룹의 리밸런싱을 시작한다.
        - 이 매개변수는 **heartbeat.interval.ms와 밀접한 관계**가 있다. 컨슈머의 poll()메서드에서 그룹조정자에게 하트비트를 전송하는 시간 간격을 제어하는 것이 heartbeat.interval.ms다. 이와는 달리 session.timeout.ms는 컨슈머가 하트비트를 전송하지 않고 살아있을 수 있는 시간이다. 그러므로 두 매개변수의 값은 같이 고려하여 설정해야 한다. (대개 session.timeout.ms 값의 1/3으로 설정)
    5. **auto.offset.reset**
        - **커밋된 오프셋이 없는 파티션을 컨슈머가 읽기 시작할 때 또는 커밋된 오프셋이 있지만 유효하지 않을 때** 컨슈머가 어떤 레코드를 읽게 할 것인지 제어하는 매개변수
        - 기본값은 유효한 오프셋이 없음을 의미하는 **latest**이며 이 경우 컨슈머는 가장 최근의 레코드들을 읽기 시작한다. (실행 후 새로 추가되는 레코드들)
        - 다른 값으로는 **earliest**가 있으며 이 경우 컨슈머는 해당 파티션의 맨 앞부터 모든 데이터를 읽게 된다. (중복가능, 누락 최소화)
    6. **enable.auto.commit**
        - **컨슈머의 오프셋 커밋을 자동으로 할 것인지 제어** 기본 true
        - 오프셋의 커밋을 컨슈머가 제어하고 싶다면 false로 설정
        - **true 일때는 auto.commit.interval.ms를 설정하여 자동으로 오프셋을 커밋하는 시간 간격을 제어**할 수 있다.
    7. **partition.assignment.strategy**
        - 컨슈머 그룹의 **각 컨슈머에게 토픽의 각 파티션을 어떻게 할당할 지** 결정하는 매개변수 (원하는 클래스 이름을 지정해준다.)
        - 카프카에서는 이런 클래스가 두개 있으며 두가지 전략을 구현한다. (커스텀 클래스를 사용할 수도 있다.)
            1. **범위**
                - 컨슈머들이 구독하는 모든 토픽의 파티션들을 각 컨슈머마다 연속적으로 할당한다.
            2. **라운드 로빈**
                - 구독하는 모든 토픽의 모든 파티션들을 컨슈머들에게 하나씩 번갈아 차례대로 할당한다.
                - 이 전략을 사용하면 모든 컨슈머가 같은 수 또는 최대 1개 차이나는 파티션을 갖게 된다.
        - 기본 값은 범위 : org.apache.kafka.clients.consumer.RangeAssignor (라운드 로빈 : org.apache.kafka.clients.consumer.RoundRobinAssignor)
    8. **client.id**
        - 클라이언트로부터 전송된 **메시지를 식별하기 위해 브로커가 사용**
    9. **max.poll.records**
        - **한번의 poll() 메소드 호출에서 반환되는 레코드의 최대 개수**를 제어
    10. **receive.buffer.bytes, send.buffer.bytes**
        - 데이터를 읽거나 쓸 때 소켓이 사용하는 TCP 송수신 버퍼의 크기를 제어한다.
7. **커밋과 오프셋**
    1. **poll**() 메서드는 호출될 때마다 그룹의 컨수머들이 **아직 읽지 않은 레코드들을 반환**한다. 카프카는 다른 많은 JMS(Java Message Service) 시스템이 하는 것과 다른 방법으로 컨슈머가 읽는 레코드를 추적 관리한다. 카프카의 **각 컨슈머는 파티션별로** 자신이 읽는 **레코드의 현재 위치**(오프셋)을 **추적 관리**할 수 있다.
    2. **파티션 내부의 현재 위치를 변경하는 것을 커밋**(commit)이라고 한다.
    3. **컨슈머가 오프셋을 커밋하면 카프카는 내부적으로 __consumer__offsets라는 이름의 특별한 토픽에 메시지를 쓴다. 이 토픽은 모든 컨슈머의 오프셋을 갖는다.** 그리고 컨슈머 그룹의 모든 컨슈머들이 정상적으로 실행중일때는 오프셋을 커밋해도 아무런 영향을 주지 않는다. 그러나 만일 기존 컨슈머가 비정상적으로 종료되거나, 새로운 컨슈머가 컨슈머 그룹에 추가 된다면 오프셋 커밋은 리밸런싱을 유발한다. 그리고 리밸런싱이 끝나면 각 컨슈머는 종전과 다른 파티션들을 할당받게 될 수 있다. 따라서 어느 위치부터 메시지를 읽어야 할지 알기 위해 컨슈머는 각 파티션의 마지막 커밋된 오프셋을 알아낸후 거기서부터 계속 읽어야 한다.
    4. **카프카 컨슈머 API에서 오프셋을 커밋하는 방법**
        1. **자동 커밋**
            - KafkaConsumer 객체가 자동으로 오프셋을 커밋
            - **enable.auto.commit = true로 설정**
            - poll() 메서드에서 **받은 오프셋 중 가장 큰 것을 한번씩 커밋**
            - **auto.commit.interval.ms로 커밋 간격 설정**
            - **오프셋 자동 커밋을 자주 하도록 시간을 설정하면 레코드의 중복을 줄일 수 있으나 완전히 없애는 것은 불가능하다.**
        2. **현재의 오프셋 커밋** (동기)
            - **enable.auto.commit = false로 설정**
            - 애플리케이션에서 요구할 때만 오프셋이 커밋
            - 가장 간단하면서 신뢰도가 높은 **commitSync**() : poll() 메소드에서 반환된 마지막 오프셋을 커밋한다. 그리고 **오프셋이 성공적으로 커밋되면 실행이 끝나지만, 만일 어떤 이유로 커밋에 실패하면 예외를 발생시킨다.**
            - commitSync()는 poll()에서 반환된 가장 최근의 오프셋을 커밋한다는 것에 유의. 따라서 **반환된 모든 레코드의 처리가 다 된 후에** commitSync()를 호출해야 한다.
            ```java
            while(true){
                ConsumerRecords<String, String> records = consumer.poll(100);
                
                for(ConsumerRecord<String, String> record : records){
                    // 데이터 처리
                }
                
                try{
                    consumer.commitSync();
                } catch (CommitFailedException e){
                    log.error("commit failed", e);
                }
            }
            ```
        3. **비동기 커밋**
            - **브로커가 커밋 요청에 응답할 때까지 애플리케이션이 일시 중지된다는 것이 수동 커밋의 한가지 단점**이다. (동기)
            - 이로 인해 애플리케이션의 처리량을 제한하게 된다. 물론 커밋을 자주하지 않으면 처리량이 증가될 수 있다. 그러나 리밸런싱으로 인해 생기는 중복 처리 레코드의 수가 증가한다.
            - **비동기 커밋 : 브로커의 커밋 응답을 기다리는 대신, 커밋 요청을 전송하고 처리를 계속할 수 있다.**
                ```java
                while(true){
                    ConsumerRecords<String, String> records = consumer.poll(100);
                
                    for(ConsumerRecord<String, String> record : records){
                        // 데이터 처리
                    }
                    
                    consumer.commitAsync();
                }
                ```
            - 커밋이 성공하거나 재시도 불가능한 에러가 생길 때까지 commitSync()는 커밋을 재시도하지만, commitAsync()는 **재시도하지 않는다는 것이 단점**이다. 왜냐하면 서버의 응답을 받는 사이에 이후의 다른 커밋이 먼저 성공할 수 있기 떄문이다.
            - 오프셋 커밋의 순서를 지키지 않을 때의 문제점과 중요성을 얘기한 이유는 브로커가 응답할 때 실행되는 **콜백**(callback)을 commitAsync()에도 전달할 수 있기 때문이다. 콜백은 커밋 에러를 로깅하거나 메트릭에서 집계하기 위해 주로 사용한다. **그러나 오프셋 커밋을 재시도하기 위해 콜백을 사용하고자 한다면 커밋 순서와 관련된 문제점을 알고 있어야 한다.**
                ```java
                while(true){
                    ConsumerRecords<String, String> records = consumer.poll(100);
                
                    for(ConsumerRecord<String, String> record : records){
                        // 데이터 처리
                    }
                    
                    consumer.commitAsync(new OffsetCommitCallback(){
                        public void onComplete(Map<TopicParition, OffsetAndMetadata> offsets, Exception e){
                            if(e != null)
                                log.error("Commit failed for offsets {}", offsets, e);
                        }
                    });
                }
                ```
            - 커밋에 실패하면 콜백 메서드인 onComplete가 자동 호출되어 에러 메시지와 해당 오프셋을 로깅한다.
            - **비동기 커밋을 재시도하기**
                - **순차적으로 증가하는 일련번호를 사용**하면 비동기 커밋을 재시도할 때 커밋 순서를 지킬 수 있다. 즉, **커밋을 할때마다 인스턴스 변수의 일련번호를 증가시키고 그것을 commitAsync 콜백에 추가로 전달**한다. 그리고 재시도 커밋을 전송할 준비가 되면 콜백이 갖고 있던 커밋 일련번호가 인스턴스 변수의 것과 같은지 확인한다. 만일 같다면 더 새로운 커밋이 없었으므로 재시도를 해도 안전하다. 그러나 인스턴스 변수의 일련번호가 더 크다면 재시도하지 말아야 한다. 더 새로운 커밋이 이미 전송된 것이기 때문이다.
        4. **동기와 비동기 커밋을 같이 사용하기**
            - 재시도 없이 오프셋 커밋이 실패해도 큰 문제가 되지 않는다. 그것이 일시적이라면 그 다음의 커밋에서 성공할 것이기 때문이다. 그러나 **폴링 루프의 실행이 끝나고 컨슈머를 닫기 전 또는 리밸런싱이 시작되기 전의 마지막 커밋이라면 성공 여부를 추가로 확인해야 한다.**
            - 이 경우 commitSync()와 commitAsync()를 같이 사용한다
            ```java
            try{
                while(true){
                    ConsumerRecords<String, String> records = consumer.poll(100);
                
                    for(ConsumerRecord<String, String> record : records){
                        // 데이터 처리
                    }
                    
                    // 정상적일 때에는 commitAsyc() 사용
                    // 처리 속도가 빠르고 하나의 커밋이 실패해도 그 다음 커밋이 재시도의 기능을 해주기 때문
                    consumer.commitAsync();
                }
            } catch (Exception e){
                log.error("Unexpected error", e);
            } finally{
                try{
                    // 컨슈머를 닫을 때는 그 다음에 수행되는 커밋이 없기 때문에 commitSync()를 호출한다.
                    // 커밋이 성공하거나 복구 불가능 에러가 될 때까지 커밋을 재시도하기 떄문
                    consumer.commitSync();
                } finally{
                    consumer.close();
                }
            }
            ```
        5. **특정 오프셋을 커밋하기**
            - 가장 최근 오프셋 커밋은 메시지 배치의 처리가 끝날 때만 수행된다. 그러나 그보다 더 자주 커밋을 하고 싶다면 또는 poll() 메서드에서 **용량이 큰 배치를 반환할 때 그 배치 중간의 오프셋을 커밋**하여 리밸런싱으로 인한 많은 메시지의 중복처리를 막고자 한다면 commitSync()나 commitAsync()의 단순한 호출로는 해결하기 어렵다. 이 메소드들은 항상 마지막으로 반환된 오프셋을 커밋하기 떄문이다.
            - 컨슈머 API에서는 우리가 커밋을 원하는 파티션과 오프셋을 저장한 Map을 인자로 전달할 수 있다.
            ```java
            // 직접 오프셋을 추적 관리하기 위한 Map
            private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
            int count = 0;
            
            while(true){
                ConsumerRecords<String, String> records = consumer.poll(100);
                for(ConsumerRecord<String, String> record : records){
                    // 데이터 처리
                    
                    // 현재의 오프셋을 map에 추가
                    // ((topic, partition), (offset))
                    currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() +1, "no metadata"));
                    
                    // 1000개의 레코드마다 한번 씩 커밋
                    if(count%1000 == 0)
                        consumer.commitAsync(currentOffsets, null);
                    count++;
                }
            }
            ```
8. **리밸런싱 리스너**
    1. **컨슈머는 종료되기 전이나 파티션 리밸런싱이 시작되기 전에 클린업하는 처리를 해야 한다.**
    2. 예를 들어, 컨슈머가 파티션의 소유권을 읽게 되는 것을 알게 된다면 처리했던 마지막 메시지의 오프셋을 커밋해야 하며, 사용하던 파일 핸들, 데이터베이스 연결 등도 닫아야 한다.
    3. **카프카 컨슈머 API에서는 컨슈머의 파티션이 추가나 제거될 때 코드가 실행될 수 있게 해준다.**
    4. subscribe() 메서드를 호출할 때 **ConsumerRebalanceListener 인터페이스를 구현하는 객체를 인자로 전달**하면 된다.
    5. ConsumerRebalanceListener 인터페이스는 두개의 메서드를 정의하고 있다.
        1. public void **onPartitionsRevoked** (Collection\<TopicPartition\> partitions)
            - 이 메서드는 **리밸런싱이 시작되기 전**에, 그리고 컨슈머가 메시지 소비를 중단한 후 호출된다.
            - **오프셋을 커밋해야하는 곳이 바로 이 메서드**다.
            - 그래야만 현재의 파티션을 이어서 소비할 다른 컨슈머가 해당 파티션의 어디서부터 메시지 소비를 시작할지 알 수 있기 때문이다.
        2. public void **onPartitionsAssigned** (Collection\<TopicPartition\> partitions)
            - 이 메서드는 **파티션이 브로커에게 재할당된 후**에, 그리고 컨슈머가 파티션을 새로 할당받아 메시지 소비를 시작하기 전에 호출된다.
        3. 리밸런싱이 시작되기 전에 커밋하기
            ```java
            private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();
            
            private class HandleRebalance implements ConsumerRebalanceListener {
                public void onPartitionsAssigned(Collection<TopicPartition> partitions){}
                
                public void onPartitionsRevoked(Collection<TopicPartition> partitions){
                    System.out.println("Lost partitions in rebalance. Committing current offsets : "+currentOffsets);
                    
                    // 이 컨슈머가 소비하던 모든 파티션의 오프셋을 커밋
                    consumer.commitSync(currentOffsets);
                }
            }
            
            try{
                // subscribe 인자로 전달
                consumer.subscribe(topics, new HandleRebalance());
                
                while(true){
                    ConsumerRecords<String, String> records = consumer.poll(100);
                    for(ConsumerRecord<String, String> record : records){
                        // 데이터 처리

                        currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() +1, "no metadata"));

                        consumer.commitAsync(currentOffsets, null);
                    }
                }
            } catch(WakeupException e){
                // 컨슈머를 닫을 것이므로 이 예외는 무시
            } catch(Exception e){
                log.error("Unexpected error", e);
            } finally{
                try{
                    consumer.commitSync(currentOffsets);
                } finally{
                    consumer.close();
                }
            }
            ```
9. **역직렬처리기** (deserializer) (**Detailed** : can be found in each directory.)
    - 카프카 프로듀서는 카프카로 전송될 메시지 객체를 바이트 배열로 변환하는 직렬처리기가 필요하다.
    - 이와는 반대로 **카프카 컨슈머는 카프카로부터 받은 바이트 배열을 자바 객체로 변환하는 역직렬처리기가 필요**하다.
    - 카프카에 쓰는 메시지를 생성하기 위해 사용되는 **직렬처리기**는 메시지를 읽을 때 사용되는 **역직렬처리기와 호환되는 것이어야 한다.**
    - 직렬화와 역직렬화를 위해 Avro와 스키마 레지스트리를 사용하면 좋은 이유가 바로 그 때문이다.
    - **Avro 직렬처리기를 사용하면 특정 토픽에 쓰는 모든 데이터가 해당 토픽의 스키마와 호환될 수 있으므로 직렬화를 어떻게 했는지 신경 쓰지 않고, Avro 역직렬처리기와 스키마로 역직렬화할 수 있다.**
    - 커스텀 직렬처리기와 역직렬처리기를 직접 구현하는 것은 바람직하지 않다.
10. **독자 실행 컨슈머** (Independent Consumer) (**Detailed** : can be found in each directory.)
    - 그룹 없이 하나의 컨슈머만 사용
    - 한 토픽의 모든 파티션이나 하나의 특정 파티션 데이터를 항상 하나의 컨슈머가 읽는다.
    - **컨슈머 그룹이나 리밸런싱이 필요 없으므로, 해당 컨슈머 전용의 토픽과 파티션을 할당한 후 메시지를 읽고 오프셋을 커밋하면 된다.**
    - 컨슈머가 어떤 파티션을 읽어야 하는지 정확하게 알고 있을 때는, **토픽을 구독하지 않는 대신 스스로 파티션을 할당**할 수 있다. **그러나 두 가지를 같이 할 수는 없다.**
    - 이 경우 **리밸런싱이 자동으로 되지 않고 직접 파티션을 찾아야 하지만 이외에는 여느 때와 다르지 않다.**
    - 단, **토픽에 새로운 파티션이 추가되어도 해당 토픽의 모든 파티션을 사용하는 컨슈머에게 알려주지 않는다**는 것에 유의해야 한다.
    - 따라서 이때는 **consumer.partitionsFor**() 메서드를 **정기적으로 호출**하여 파티션 정보를 확인한 후 처리해야 한다. 또는 파티션이 추가될 때마다 간단히 애플리케이션을 수정해서 처리해도 된다.
    
----------------------------------------------------

## Kafka internal mechanism
1. **클러스터 맴버십**
    1. **주키퍼**
        - **내부적으로 주키퍼는 표준 파일 시스템의 디렉터리처럼 계층적인 트리 구조로 데이터를 저장하고 사용한다.**
        - **데이터를 저장하는 노드를 znode**라고 하며, **각 znode의 이름 앞에 /를 붙이고 디렉터리처럼 경로를 사용해서 각 노드의 위치를 식별**한다.
        - 예를 들어, 최상위의 루트 znode인 / 밑에 kafka-main을 생성하고 이것의 자식 노드로 kafka-sub1과 kafka-sub2를 생성하면, kafka-sub1의 경로는 /kafka-main/kafka-sub1, kafka-sub2의 경로는 /kafka-main/kafka-sub2가 된다.
        - **노드 관리는 주키퍼를 사용하는 클라이언트에서 한다.**
        - 예를 들어, 노드의 생성/삭제, 해당 경로의 노드 존재 여부 확인, 해당 노드의 데이터를 읽거나 쓰기, 특정 노드의 모든 자식 노드 내역 가져오기 등이다.
        - **각 노드에는 상태와 구성 정보 및 위치 정보 등의 데이터만 저장**되므로 크기가 작으며, 모든 노드가 메모리에 저장되어 처리되므로 속도가 빠르다.
        - **임시 노드**(ephemeral)**와 영구 노드**(persistent)로 구분된다.
            - **임시 노드**는 노드를 생성한 클라이언트가 연결되어 있을 때만 존재하며, 연결이 끊어지면 자동으로 삭제된다.
            - **영구 노드**는 클라이언트가 삭제하지 않는 한 계속 보존된다.
        - 또한 **노드의 상태를 모니터링하는 Watch 기능**이 있다. 즉 클라이언트가 특정 노드의 Watch를 설정한 경우 해당 노드가 변경되면 콜백 호출을 통해 클라이언트에게 알려준다.
        - **카프카에서도 주키퍼를 사용해서 클러스터 멤버인 브로커들의 메타데이터를 유지 관리**한다.
        - 예를 들어, 카프카 클러스터가 사용하는 주키퍼의 최상위 노드가 /kafka-main 이라면 이 노드의 자식 노드로 /kafka-main/controller, /kafka-main/brokers, /kafka-main/config가 생성된다.
            - **/kafka-main/controller** 에는 카프카 클러스터의 컨트롤러 정보가 저장
            - **/kafka-main/brokers** 에는 브로커 관련 정보가 저장 (/brokers/ids에는 브로커 ID가, /brokers/topics 에는 토픽 정보가 저장)
            - **/kafka-main/config** 에는 토픽의 설정 정보가 저장
            - 이외에도 컨슈머의 파티션 오프셋 정보를 /kafka-main/consumers에 저장했지만, 카프카 0.9버전부터는 __consumer_offsets라는 이름의 특별한 카프카 토픽에 저장하도록 변경되어 이 노드를 사용하지 않는다.
        - **모든 카프카 브로커는 고유 식별자를 가지며, 이것은 브로커의 구성 파일에 설정되거나 자동으로 생성된다.**
        - 브로커 프로세스는 시작될 때마다 주키퍼의 /brokers/id에 임시 노드로 자신의 ID를 등록한다.
        - **만일 같은 ID를 갖는 다른 브로커를 시작하려고 하면 같은 주키퍼 노드가 이미 있으므로 에러가 발생한다.**
        - 그리고 브로커 구성에서 브로커가 추가 혹은 삭제되거나 주키퍼와 연결이 끊어지면, 해당 브로커가 시작될 때 생성되었던 임시 노드는 자동으로 주키퍼에서 삭제되며, 브로커 내역을 모니터링하는 카프카 컴포넌트들이 알수 있다.
        - 브로커가 중단되면 해당 브로커의 주키퍼 노드는 삭제된다. 그러나 해당 브로커의 ID는 여전히 다른 데이터 구조에 존재한다.
        - 예를 들어, 각 토픽의 리플리카 내역에는 브로커들의 ID가 포함되어 있다. 따라서 **중단으로 누락된 브로커 ID로 완전히 새로 교체한 브로커를 시작시키면, 누락된 브로커에 할당되었던 파티션과 토픽을 가지고 클러스터에 합류하게 된다.**
    2. **컨트롤러**
        - **컨트롤러는 카프카 브로커 중 하나**이며, 일반 브로커의 기능에 추가하여 **파티션 리더를 선출하는 책임**을 갖는다.
        - **클러스터에서 시작하는 첫번째 브로커가 컨트롤러가 되며, 이 브로커는 주키퍼의 임시노드인 /controller를 생성한다.**
        - **다른 브로커들이 시작될 때도 /controller 노드를 생성하려고 시도**한다. 그러나 **노드가 이미 존재한다는 예외**를 받으므로, 이미 /controller 노드가 있고 클러스터에도 컨트롤러가 있다는 것을 알게 된다.
        - **모든 브로커들은 /controller 노드에 주키퍼의 Watch를 생성**하므로 이 노드에 변경이 생기는 것을 알 수 있다. 이런 방법을 통해서 클러스터에는 항상 하나의 컨트롤러만 존재한다.
        - **컨트롤러 브로커가 중단되거나 주키퍼와의 연결이 끊어지면 임시 노드인 /controller가 삭제**된다.
        - 이때 **해당 클러스터의 다른 브로커들이 주키퍼의 Watch를 통해 그 사실을 알게 되고 /controller 노드의 생성을 시도**한다. 그리고 그 노드를 첫번째로 생성한 브로커가 컨트롤러가 되며, 다른 브로커들은 이미 얘기한대로 노드가 이미 존재한다는 예외를 받는다. 또한 모든 브로커가 새로 생성된 /controller 노드에 주키퍼의 Watch를 생성한다.
        - **컨트롤러는 매번 새로 선출될 때마다 주키퍼로부터 새로운 컨트롤러 세대 번호를 받으며, 나머지 브로커들은 현재의 컨트롤러 세대 번호를 알게 된다.**
        - **따라서 변경 전의 컨트롤러와 혼동되지 않으며, 이전 세대 번호로 된 컨트롤러 메시지를 받으면 무시한다.**
        - 관련 주키퍼 경로를 Watch 하여 **특정 브로커가 클러스터를 떠났다는 것을 컨트롤러가 인지**한다면, **그 브로커가 리더로 할당되었던 모든 파티션들에 새로운 리더가 필요**하다는 것을 알게 된다.
        - 그 다음에 **컨트롤러는 새로운 리더를 필요로 하는 모든 파티션들을 점검**하고, **새로 리더가 될 브로커를 결정**한다. (**간단히 해당 파티션의 리플리카 리스트에서 그다음 순서의 브로커로 결정**)
        - 그리고 **컨트롤러는 파티션들의 새로운 리더들과 팔로어 들의 정보를 모든 브로커들에게 전송**한다.
        - **새로 결정된 각 파티션의 리더는 프로듀서와 컨슈머의 요청 처리를 시작해야 한다는 것을 알고 있으며, 팔로어들은 새로운 리더의 메시지 복제를 시작해야 한다는 것을 안다.**
        - **새 브로커가 클러스터에 추가되면 컨트롤러는 브로커 ID를 사용해서 그 브로커의 리플리카로 사용할 브로커가 있는지 확인한다. 만일 있으면 컨트롤러는 새 브로커와 기존 브로커 모두에게 변경 사항을 알리며 새 브로커의 리플리카들은 기존 리더들의 메시지를 복제하기 시작한다.**
    3. **요약**
        - **카프카는 주키퍼의 임시 노드를 사용해서 컨트롤러를 선출**한다.
        - **브로커가 추가 또는 중단되어 임시 노드가 추가되거나 삭제될 때 주키퍼의 Watch를 통해 모든 브로커가 노드의 변경을 알 수 있기 때문**이다.
        - **컨트롤러는 리더를 선출하는 책임**을 갖는다.
        - 그리고 **주키퍼가 부여한 세대 번호를 컨트롤러에 사용하므로 변경 전의 컨트롤러와 혼동되는 것을 막고 현재의 컨트롤러를 인식**할 수 있다.
2. **복제**
    1. **복제**
        - **카프카 아키텍처의 핵심**
        - **어쩔 수 없이 각 서버 노드에 장애가 생길 때 카프카가 가용성과 내구성을 보장하는 방법**
        - 카프카 데이터는 토픽으로 구성되며, 각 토픽은 여러 파티션에 저장될 수 있다. 또한, 각 파티션은 다수의 리플리카를 가질 수 있다.
        - 그리고 각 브로커는 서로 다른 토픽과 파티션에 속하는 수백에서 많게는 수천개까지의 복제본을 저장한다.
    2. **리플리카의 형태**
        1. **리더** 리플리카
            - **각 파티션은 리더로 지정된 하나의 리플리카를 갖는다. 일관성을 보장하기 위해 모든 프로듀서와 컨슈머 클라이언트의 요청은 리더를 통해서 처리된다.**
        2. **팔로어** 리플리카
            - 각 파티션의 **리더를 제외한 나머지 리플리카를 팔로어**라고 한다.
            - 팔로어는 **클라이언트 요청을 서비스하지 않는다. 대신에 리더의 메시지를 복제하여 리더의 것과 동일하게 유지**한다.
            - 그리고 특정 파티션의 **리더 리플리카가 중단되는 경우에는 팔로어 리플리카 중 하나가 해당 파티션의 새로운 리더로 선출**된다.
    3. **매커니즘**
        - **리더는 팔로어 리플리카 중에 어느 것이 최신의 리더 메시지를 복제하고 있는지 알아야 한다.**
        - **팔로어들은 리더가 받는 모든 최신 메시지를 복제하려고 한다.** 그러나 여러가지 이류로 동기화에 실패할 수 있다.
        - 예를 들어, 네트워크의 혼잡으로 복제가 늦어지는 경우, 또는 브로커가 중단되어 해당 브로커를 시작 시킨 후 다시 복제를 시작할 수 있을 때까지 해당 브로커의 모든 리플리카들의 복제가 늦어지는 경우다.
        - **리더와 동기화를 하기 위해 리플리카들은 리더에게 Fetch 요청을 전송**한다. (컨슈머가 메시지를 읽기 위해 전송하는 것과 같은 타입)
        - **Fetch 요청에는 리플리카가 다음으로 받기 원하는 메세지의 오프셋이 포함되며, 항상 수신된 순서대로 처리**된다.
        - 각 팔로어 리플리카가 요청한 마지막 오프셋을 살펴보면 복제가 얼마나 지연되고 있는지 리더가 알 수 있다.
        - 만일 팔로어 리플리카가 10초 이상 메시지를 요청하지 않았거나 요청은 했지만 10초 이상의 시간 동안에도 가장 최근의 메시지를 복제하지 못했다면, 해당 리플리카는 동기화되지 않은 것으로 간주한다. 그리고 이처럼 **팔로어 리플리카가 리더를 복제하는 데 실패하면, 리더에 장애가 생겼을 때 해당 리플리카는 더 이상 새로운 리더가 될 수 없다. 따라서 결국 모든 메시지를 갖지 못하게 될 것이다.**
        - 이와는 반대로 **최신 메시지를 계속 요청하는 팔로어 리플리카를 동기화 리플리카**(in-sync replica, **ISR**)이라고 한다. 기존 리더가 중단되는 경우 **동기화 리플리카만이 리더로 선출될 수 있다.**
        - **동기화되지 않는다고 간주되기 전에, 팔로어가 비활성 상태가 될 수 있는 지연시간은 replica.lag.time.max.ms 구성 매개변수로 제어**할 수 있다. 이 지연 시간은 리더를 선출하는 동안의 클라이언트 실행과 데이터 보존에 영향을 준다.
        - 현재 리더에 추가하여 각 파티션은 **선호 리더**를 갖는다. 이것은 **토픽이 생성될 때 각 파티션의 리더였던 리플리카**들을 말한다.
        - 하나의 토픽은 여러 개의 파티션으로 구성될 수 있으며, 파티션들을 처음 생성할 때는 여러 브로커가 고르게 파티션을 할당받아 리더가 되므로 이것이 선호되는 리더들이다.
        - 결론적으로 선호 리더들이 클러스터의 모든 파티션 리더들일 때는 브로커 간의 파티션 배분이 고르게 된다.
        - **기본적으로 카프카는 auto.leader.rebalance.enable=true로 구성**된다. 이 경우 **선호 리더 리플리카가 현재 리더는 아닐 경우 동기화 리플리카인지 확인**한다. 그리고 **리더를 선출할 때 선호 리더를 현재 리더로 선출**한다.
        - 파티션의 리플리카 내역을 보면 현재의 선호 리더를 찾을 수 있다. 출력 내역의 첫 번째 리플리카가 항상 선호 리더이며, 이것은 어떤 리플리카가 현재 리더라도 마찬가지다. 또한 리플리카 재지정 도구를 사용해서 다른 브로커로 리플리카를 재지정했어도 그렇다. 만일 우리가 직접 리플리카를 재지정한다면 제일 먼저 지정하는 리플리카가 선호 리더라는 것을 기억하자. 이렇게 하면 같은 파티션의 리더가 여러 브로커로 중복으로 지정되는 것을 막을 수 있다.
3. **요청 처리**
    1. **요청 처리**
        - 카프카 브로커가 하는 일은 대부분 클라이언트와 파티션 리플리카 및 컨트롤러로부터 파티션 리더에게 전송되는 요청을 처리하는 것이다.
        - 카프카는 TCP로 전송되는 이진 프로토콜을 갖고 있다. 이 프로토콜은 요청의 형식을 규정하고 있으며, 또한 요청이 성공적으로 처리되거나 처리 중 에러가 발생할 때 브로커가 응답하는 방법도 나타낸다.
        - 클라이언트는 항상 연결을 개시하고 요청을 전송하며, 브로커는 해당 요청을 처리하고 응답한다.
        - **특정 클라이언트로부터 브로커에 전송된 모든 요청은 항상 수신된 순서로 처리**된다. 따라서 카프카가 메시지 큐처럼 동작할 수 있어서 저장되는 메시지의 순서가 보장된다.
        - **모든 요청은 다음 내용을 포함하는 표준 헤더를 갖는다.**
            1. **요청 타입 ID** : 어떤 요청인지를 나타내며, 16비트 정수형식의 고유 번호이다. 예를 들어, 카르카에 메시지를 쓰는 요청은 Produce라고 하며 이것의 ID 값은 0이고, 메시지를 읽는 요청은 Fetch라고 하며 이것의 ID 값은 1이다.
            2. **요청 버전** : 이 요청의 프로토콜 API 버전을 나타내는 16비트 정수값이다. 따라서 서로 다른 프로토콜 버전을 사용하는 카프카 클라이언트를 브로커가 처리하고 그에 맞춰 응답할 수 있다.
            3. **cID** (correlation ID) : 사용자가 지정한 32비트 정숫값이며, 이것을 각 요청의 고유 식별 번호로 사용하면 문제를 해결하는 데 도움이 될 수 있다. 이 값은 응답과 에러 로그에도 표시된다.
            4. **클라이언트 ID** : 사용자가 지정한 문자열 형식의 값이며 null이 될 수 있다. 이것은 요청을 전송한 클라이언트 애플리케이션을 식별하는 데 사용될 수 있다.
        - 그리고 **표준 헤더에 추가하여 요청 타입마다 서로 다른 구조의 데이터를 같이 전송**한다.
        - 예를 들어, 카프카에 메시지를 쓰는 Produce 요청의 경우에는 토픽 이름과 파티션 ID 및 데이터 등이 포함된다.
        - 브로커는 자신이 리스닝하는 각 포트에 대해서 acceptor 스레드를 실행하며, 이 스레드는 연결을 생성하고 processor 스레드의 개수는 우리가 구성할 수 있다.
        - processor 스레드는 클라이언트 연결로부터 요청을 받고, 그것을 요청 큐에 넣으며 응답 큐로부터 응답을 가져와서 클라이언트에게 전송하는 일을 수행한다.
    2. **읽기 / 쓰기 요청**
        - **쓰기 요청과 읽기 요청은 모두 파티션의 리더 리플리카에게 전송되어야 한다.** 만일 어떤 브로커가 특정 파티션의 쓰기 요청을 수신했는데 해당 파티션의 리더가 다른 브로커라면, 쓰기 요청을 전송한 클라이언트는 파티션 리더가 아님이라는 에러 응답을 받는다.
        - 또한 특정 파티션의 리더를 갖고 있지 않은 브로커가 해당 파티션의 읽기 요청을 수신할 때도 같은 에러가 발생한다. **카프카의 클라이언트들이 쓰기와 읽기 요청을 할 때는 요청 관련 파티션 리더를 포함하는 브로커에게 요청을 전송해야 한다.**
        - 카프카 클라이언트는 **메타데이터 요청**이라는 또 다른 요청 타입을 사용하는데, 이것은 클라이언트가 관심을 갖는 토픽 내역을 포함한다.
        - 그리고 **메타데이터 요청에 대한 서버 응답에는 토픽에 존재하는 파티션들, 각 파티션의 리플리카, 어떤 리플리카가 리더인지 등의 정보가 포함**된다.
        - **메타데이터 요청은 어떤 브로커에도 전송할 수 있다. 모든 브로커가 그런 정보를 포함하는 메타데이터 캐시를 갖고 있기 때문**이다.
        - **대개 클라이언트는 그런 정보를 캐시에 보존**한 후 각 파티션의 올바른 브로커에게 쓰기와 읽기 요청을 전송하는 데 사용한다.
        - 또한 메타데이터 요청을 전송하여 가끔 그런 정보를 새로 **교체**해야 한다. (**새로 교체하는 시간 간격은 metadata.max.age.ms 구성 매개변수로 제어**) 그래야만 새로운 브로커가 추가되었을 때처럼 토픽 메타데이터가 변경된 것을 알 수 있기 때문이다.
        - 또한 클라이언트는 자신의 요청 중 하나에서 파티션 리더가 아님 에러를 수신할 때도 해당 요청을 다시 전송하기 전에 메타데이터를 새로 교체한다. 왜냐하면 클라이언트가 낡은 정보를 사용하여 엉뚱한 브로커에게 요청을 전송한 것이기 때문이다.
    3. **쓰기 요청**
        - **acks 구성 매개변수에는 메시지를 수신해야 하는 브로커의 수를 설정**하며, **설정된 값의 브로커가 모두 메시지를 수신해야 쓰기 성공으로 간주**한다.
        - 프로듀서의 acks 매개변수는 다음 세가지 중 하나로 설정할 수 있다.
            1. **acks=1** : 리더만 메시지를 받으면 됨.
            2. **acks=all** : 모든 동기화 리플리카가 메시지를 받아야 함.
            3. **acks=0** : 아예 브로커의 수신 응답을 기다리지 않음.
        - 특정 파티션의 리더 리플리카를 포함하는 브로커가 해당 파티션의 쓰기 요청을 받으면 다음 사항의 검사를 시작한다.
            1. 데이터를 전송한 사용자가 **해당 토픽의 쓰기 권한**을 가지고 있는가?
            2. 해당 요청에 지정된 **acks의 값이 적합**한가?
            3. 만일 acks가 all로 설정되었다면 메시지를 안전하게 쓰는 데 **충분한 동기화 리플리카들이 있는가**?
        - 그다음에 브로커는 로컬 디스크에 새로 받은 메시지를 쓴다.
        - 리눅스의 경우에는 일단 파일 시스템 캐시에 메시지를 쓰지만 언제 디스크에 쓰는지 알 수 없다.
        - 카프카는 디스크에 데이터를 쓰기를 기다리지 않으며, 메시지의 내구성 보장을 위해 복제에 의존한다.
        - 일단 파티션 리더에 메시지를 쓰면 브로커는 acks 구성 매개변수를 살펴본다. 만일 acks의 값이 0 또는 1이면 즉시 응답을 전송한다. 그렇지 않고 all이면 팔로어 리플리카들이 해당 메시지를 복제했는지 리더가 확인할 때까지 **퍼거토리**(purgatory)라고 하는 버퍼에 해당 요청을 저장한다.
    4. **읽기 요청**
        - 브로커는 쓰기 요청을 처리하는 방법과 매우 유사하게 읽기 요청을 처리한다.
        - **클라이언트는 읽기를 원하는 토픽과 파티션 및 오프셋에 있는 메시지들의 읽기 요청을 브로커에게 전송**한다.
        - **클라이언트는 각 파티션마다 브로커가 반환할 수 있는 데이터 크기를 제한할 수 있다.**
        - 클라이언트는 **브로커가 전송한 응답을 저장하는 메모리를 할당해야 하므로 데이터 크기 제한이 중요**하다. 크기를 제한하지 않으면 클라이언트의 메모리 부족을 초래할 만큼 큰 응답을 브로커가 전송할 수 있다.
        - **각 요청은 파티션 리더에게 전송되어야 한다. 따라서 읽기 요청이 올바르게 전달되도록 클라이언트는 필수적인 메타데이터 요청을 한다.**
        - 리더는 요청을 받은 후 그것이 적합한지 제일 먼저 검사한다.
            - 만일 너무 오래되어서 해당 파티션에서 이미 삭제된 메시지나 아직 존재하지 않는 오프셋을 클라이언트가 요청하면 브로커는 에러를 응답한다.
            - 그러나 오프셋이 존재하면, 클라이언트가 요청에 지정한 제한 크기까지의 메시지들을 브로커가 해당 파티션에서 읽은 후 클라이언트에게 전송한다.
        - **카프카는 제로카피 기법을 사용해서 클라이언트에게 메시지를 전송**한다. 즉, **파일의 메시지를 중간 버퍼 메모리에 쓰지 않고 곧바로 네트워크 채널로 전송한다는 의미**다.
        - 이것은 데이터를 클라이언트에게 전송하기 전에 로컬 캐시 메모리에 저장하는 대부분의 데이터베이스와 다르다.
        - **제로카피 기법은 메모리로부터 데이터를 복사하고 버퍼를 관리하는 부담을 제거하므로 성능이 훨씬 더 향상된다.**
        - 브로커가 반환할 수 있는 데이터의 상한 크기를 설정하는 것과 더불어, 클라이언트는 **반환 데이터의 하한 크기도 설정할 수 있다.**
            - 이것은 트레픽이 그리 많지 않은 토픽으로부터 클라이언트가 메시지를 읽을 때 CPU와 네트워크 사용을 줄일 수 있는 좋은 방법이다.
            - 즉, 반환되는 데이터가 거의 없는데도 수밀리초마다 클라이언트가 브로커에게 요청을 전송하는 대신, **클라이언트는 요청을 전송하고 브로커는 하한크기만큼 데이터가 채워지기를 기다렸다가 전송**한다.
            - 그리고 그다음에 클라이언트가 추가로 읽기 요청하면 된다. 결국 전체적으로는 같은 양의 데이터를 읽는다. 그러나 클라이언트와 브로커 간에 데이터를 주고받은 횟수가 훨씬 줄어들게 되므로 여러 면에서 부담이 줄어든다.
        - 브로커가 충분한 데이터를 가질 때까지 클라이언트가 기다리는 것 대신에 잠시 기다렸다가 바로 데이터를 받아서 처리하는 것이 좋다. 이때 클라이언트는 **타임아웃**을 지정할 수 있다.
        - **파티션 리더에 존재하는 모든 데이터를 클라이언트가 읽을 수 있는 것은 아님**을 알아두자. 대부분의 클라이언트는 **모든 동기화 리플리카에 쓴 메시지들만 읽을 수 있다.**
            - 파티션 리더는 어떤 메시지들이 어느 리플리카에 복제되었는지 안다. 그리고 **모든 동기화 리플리카들이 메시지를 쓸 때까지는 컨슈머에게 전송되지 않는다.** (이 시간동안에 컨슈머가 그런 메시지를 읽으려고 하면 에러가 아닌 빈 응답이 전송된다.)
            - **리플리카들에게 아직 복제되지 않은 메시지들은 불안전한 것으로 간주하기 때문**이다.
            - 만일 리더가 중단되어 다른 리플리카가 리더로 선출되면, 모든 리플리카에 복제되지 않은 메시지들은 더 이상 카프카에 존재하지 않게 된다. 또한 리더에만 존재하는 메시지들을 클라이언트가 읽을 수 있게 한다면, 일관성이 결여될 수 있다.
            - 예를 들어, 어떤 컨슈머가 아직 복제되지 않은 메시지를 읽은 후 리더가 중단되면, 해당 메시지를 갖는 다른 브로커가 없으므로 그 메시지는 사라지게 된다. 그리고 다른 컨슈머들도 그 메시지를 읽을 수 없게 되어 그 메시지를 읽었던 컨슈머와의 일관성이 결여된다.
            - 따라서 모든 동기화 리플리카가 해당 메시지를 복제할 때까지 기다렸다가 복제된 다음에 컨슈머가 읽을 수 있게 하는 것이다.
        - 그러나 브로커 간의 메시지 복제가 어떤 이유로든 느리게 수행된다면 복제가 끝날 때까지 기다려야 하므로 컨슈머들이 메시지를 읽는 시간도 더 오래 걸릴 것이다. 이런 **지연 시간은 replica.lag.time.max.ms 매개변수로 제한**할 수 있다. 이 매개변수에 **리플리카가 살아있는 것으로 간주되는 동안 새로운 메시지의 복제에 소요될 수 있는 제한시간**을 설정한다.
    5. **기타 요청들**
        - **카프카 브로커들간에 사용하는 요청**, 이것들은 내부적인 것이라서 클라이언트에서는 사용하지 않는다.
        - 예를 들어, **특정 파티션이 새로운 리더를 갖는다는 것을 컨트롤러가 알릴 때는 새 리더와 팔로어들에게 LeaderAndIsr 요청을 전송**한다. 이제부터는 클라이언트 요청을 받아야 한다는 것을 새 리더가 알아야 하고, 팔로어들은 새 리더를 복제해야 한다는 것을 알아야 하기 때문이다.
        - 과거에 카프카 컨슈머는 아파치 주키퍼를 사용해서 카프카로부터 받는 오프셋들을 저장하였다. 따라서 컨슈머가 시작될 때는 메시지를 읽을 위치를 알기 위해 주키퍼의 오프셋들을 확인해야 했다. 그러나 새로운 카프카 버전에서는 주키퍼를 사용하는 대신에 카프카의 특별한 토픽에 오프셋들을 저장한다. 그리고 이에 따라 OffsetCommit, OffsetFetch, ListOffSets 요청들이 프로토콜에 새로 추가되었다. 따라서 이제는 클라이언트 API를 사용해서 컨슈머 오프셋을 커밋하면, 더 이상 주키퍼를 쓰지 않고 대신에 OffsetCommit 요청이 카프카에 전송된다.
        - 버전 업그레이트는 카프카 브로커와 프로토콜 요청 및 클라이언트 API 모두에 지속적으로 수행된다. 그러므로 업그레이드된 버전이 나왔을 때 현재 사용 중인 카프카와 클라이언트를 업그레이드 할떄 유의할 것이 있다. 즉, **클라이언트 버전을 업그레이드하기 전에 브로커 버전을 먼저 업그레이드할 것을 권한다**. 새로운 버전의 브로커는 구버전의 프로토콜 요청 처리방법을 알지만 이와 반대로 구버전의 브로커는 신버전의 프로토콜 요청을 처리할 수 없기 때문이다.
4. **스토리지**
    1.  **스토리지**
        - 카프카의 **기본적인 스토리지 단위는 파티션 리플리카**다.
        - **하나의 파티션은 여러 브로커 간에 분할될 수 없다.** 따라서 하나의 파티션 크기는 단일 마운트 포인트에 사용 가능한 공간으로 제한된다.
        - **카프카를 구성할 때 관리자는 파티션이 저장될 디렉터리 내역을 log.dirs 매개변수에 지정**한다. (**에러 로그는 log4j.properties 파일에 설정**)
    2. **파티션 할당**
        - **토픽을 생성할 때 카프카는 제일 먼저 여러 브로커 간에 파티션을 할당하는 방법을 결정**한다.
        - 예를 들어, 6개의 브로커가 있고 토픽에 10개의 파티션을 생성하며, 복제 팩터는 3으로 한다고 가정해보자. 이 경우 카프카는 6개의 브로커에게 할당할 30개의 파티션 리플리카를 갖는다. 파티션 할당은 다음과 같이 할 것이다.
            1. **파티션 리플리카들을 브로커 간에 고르게 분산**시킨다. 따라서 여기서는 브로커당 5개의 리플리카를 할당한다.
            2. **각 파티션의 리플리카는 서로 다른 브로커에 할당**한다. 예를 들어, 브로커 2가 파티션 0의 리더를 갖는다면, 팔로어들은 브로커 2와3이 아닌 3과 4에 할당한다.
            3. 만일 브로커가 랙(rack) 정보(카프카 0.10.0 이상 버전에서 가능)를 갖고 있다면, 가능한 한 각 파티션의 리플리카는 서로 다른 랙에 있는 것으로 지정한다. **이렇게 하면 하나의 랙 전체가 작동하지 않더라도 모든 파티션을 사용하지 못하게 만드는 불상사가 생기지 않기 때문이다.**
        - 이렇게 하기 위해 임의의 브로커(예를 들어, 4)부터 시작한다. 그리고 각 브로커에 파티션 할당을 시작하며, 이때 각 파티션의 리더를 결정하기 위해 라운드로빈 방식을 사용한다.
        - 따라서 파티션 0의 리더는 브로커 4가되고, 파티션 1의 리더는 브로커 5. 파티션 2의 리더는 브로커 0 등이 된다. 그 다음에 각 파티션의 팔로어를 결정한다.
        - 이때 리더 브로커의 번호보다 증가된 번호를 갖는 브로커를 차례대로 팔로어로 지정한다. 예를 들어, 파티션 0의 리더가 브로커 4라면, 첫번째 팔로어는 브로커 5가되고 두번째 팔로어는 브로커 0이 된다. 그리고 파티션 1의 리더는 브로커 5이므로, 첫번째 팔로어는 브로커 0이 되고 두번째 팔로어는 브로커 1이 된다.
        - 이처럼 번호순으로 브로커를 선택하는 대신, **랙-인식**(**rack-aware**) 방법을 고려할 때는 서로 다른 랙의 브로커가 번갈아 선택되도록 순서를 정해야 한다. 예를 들어, 브로커 0,1,2가 같은 랙에 있고 브로커 3,4,5는 다른 랙에 있다는 것을 우리가 알고 있다고 가정해보자. 이때는 0부터 5까지의 순서로 브로커를 선택하는 대신 0,3,1,4,2,5의 순서를 사용할 수 있다. 즉, 각 브로커 다음 순서는 그것과 다른 랙에 있는 브로커가 된다. 이 경우 파티션 0의 리더가 브로커 4라면, 첫번째 팔로어는 완전히 다른 랙에 있는 브로커 2가 된다. 이것은 매우 좋은 방법이다. 왜냐하면 첫번째 랙이 오프라인이 되더라도 두번째 랙의 리플리카들은 살아있으므로 파티션은 여전히 사용가능하기 때문이다. 이것은 실제로 모든 리플리카에서 그렇다. 따라서 만에 하나 랙에 장애가 생겨도 가용성을 보장할 수 있다.
        - 각 파티션과 리플리카의 브로커가 선택되었으면 이제는 새 파티션들에 사용할 디렉터리를 결정해야 한다. 이것은 각 파티션마다 독립적으로 하면 되며, 규칙은 매우 간단하다. 각 디렉터리의 파티션 개수를 계산하고 가장 적은 수의 파티션을 갖는 디렉터리에 새 파티션을 추가하면 된다. 즉, 새로 디스크를 추가한 경우 모든 새 파티션을 그 디스크에 생성한다는 의미다. 균형이 잡힐때까지는 새 디스크가 항상 가장 적은 수의 파티션을 갖기 때문이다.
        - **브로커에 파티션을 할당할 때는 사용가능한 디스크 공간이나 기존의 사용량이 고려되지 않는다**. **또한 파티션을 디스크에 할당할 때는 파티션의 크기가 아닌 개수가 고려대상이 된다는 것에 주의해야한다**. 따라서 일부 브로커들이 다른 브로커보다 더 많은 디스크 공간을 갖게 될수 있다. 이 경우 일부 파티션들이 매우 커지거나 같은 브로커가 서로 다른 크기의 디스크들을 갖게 될 수 있으므로 파티션할당에 주의해야 한다.
    3. **파일 관리**
        - **보존은 카프카의 중요한 개념**이다. 카프카는 데이터를 영원히 보존하지 않으며 메시지 삭제 전에 모든 컨슈머가 읽기를 기다리지도 않기 때문이다. 대신에 카프카 관리자는 다음 두가지 중 하나로 각 토픽별 보존구성을 설정할 수 있다. 메시지를 삭제하기 전에 보존하는 시간, 또는 오래된 메시지의 제거전에 보존할 데이터의 크기다.
        - **큰 파일에서 제거해야 하는 메시지를 찾아 파일 일부분을 삭제하는 것은 시간이 많이 소요되고 에러도 생길 수 있다**. 따라서 **카프카에서는 각 파티션을 세그먼트로 나눈다**.(이것을 로그 세그먼트라고 한다) 기본적으로 각 세그먼트는 최대 1GB의 데이터 또는 1주일 동안 데이터를 보존한다. 카프카 브로커가 파티션에 데이터를 쓸때 세그먼트의 제한크기나 보존기간에 도달하면 해당 파일을 닫고 새로운 세그먼트 파일에 계속쓴다.
        - **메시지를 쓰기위해 사용중인 세그먼트를 액티브 세그먼트**라고 한다. **액티브 세그먼트는 삭제되지 않는다.** 따라서 만일 로그 보존기간은 1일로, 그리고 각 세그먼트는 5일 동안 보존하게 설정한다면, 데이터는 5일 동안 보존된다. 세그먼트 파일이 닫혀야먄 삭제할수있기 때문이다. 만일 1주동안 데이터를 보존하게 하고 매일 하나의 새로운 세그먼트를 생성한다면, 파티션은 7개의 세그먼트를 갖게 된다.
        - 카프카 브로커는 모든 파티션의 모든 세그먼트에 대해 각각 하나의 열린파일핸들을 유지한다. 이로 인해 열린파일핸들이 많아질 수 있으므로 운영체제의 조정이 필요하다.
    4. **파일 형식**
        - **각 세그먼트는 하나의 데이터 파일로 생성**되며, **카프카 메시지와 오프셋들이 저장**된다.
        - **디스크에 수록되는 데이터형식은 메시지의 형식과 동일**하다. 이처럼 디스크와 네트워크 모두에서 같은 메시지 형식을 사용하므로 앞의 읽기 요청에서 설명했듯이 카프카는 **제로카피 기법을 사용해서 메시지 전송을 최적화할 수 있다**. 즉, 컨슈머에게 메시지를 전송할 때 별도의 버퍼 메모리를 사용하지 않고 디스크에서 바로 네트워크로 전송하며, 프로듀서가 이미 압축해서 전송한 메시지의 압축 해제와 재압축을 하지 않아도 된다.
        - 키와 값 및 오프셋에 추가하여 각 메시지는 메시지 크기, 손상 여부를 검출하기 위한 체크섬 코드, 메시지 형식의 버전을 나타내는 매직 바이트, 압축코덱(Snappy, GZip, LZ4), 타임스탬프 등을 포함한다. 카프카의 구성에 따라 타임스탬프는 메시지가 전송될 때 프로듀서가 지정하거나 메시지가 수신될 때 브로커가 지정한다.
        - **만일 프로듀서가 압축된 메시지를 전송한다면, 하나의 배치에 포함된 모든 메시지가 같이 압축되어 래퍼 메시지의 값으로 전송**된다. 그러면 브로커가 하나의 메시지를 수신하고 컨슈머에게도 이렇게 전송하면 된다. 이 경우 컨슈머가 해당 메시지 값의 압축을 풀면 배치에 포함된 모든 메시지를 알 수 있다.
        - 프로듀서가 **압축**을 사용(**권장**)한다면 더 큰 배치를 전송해도 네트워크와 브로커 디스크 모두에서 유리하다는 의미다. 단, 컨슈머가 사용하는 메시지 형식을 변경하는 경우에는 (예를 들어, 타임스탬프를 메시지에 추가함) 전송 프로토콜과 디스크 수록 형식 모두 변경해야 하며, 업그레이드로 인해 두 가지 형식을 갖게 된 메시지들을 포함하는 파일 처리방법을 카프카 브로커가 알아야 한다.
        - 카프카 브로커는 **DumpLogSegment** 도구와 함께 배포된다. 이것을 사용하면 **파일 시스템에서 파티션 세그먼트와 그것의 내용을 자세히 살펴볼 수 있으며, 각 메시지의 오프셋 체크섬, 매직바이트, 크기, 압축코덱도 볼 수 있다.**
            ```bash
            bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --files /tmp/kafka-logs/TopicName-0/*.log
            ```
    5. **인덱스**
        - 카프카는 컨슈머가 특정 오프셋부터 메시지를 읽을 수 있게 해준다.
        - 예를 들어, 컨슈머가 오프셋 100에서 시작하는 1MB 메세지를 요청하면, 브로커가 오프셋 100의 메시지를 빨리 찾아서 그 오프셋부터 메시지를 읽기 시작한다. 이때 지정된 오프셋의 메시지를 브로커가 빨리 찾을 수 있도록 카프카는 각 파티션의 인덱스를 유지 관리하며, 인덱스는 세그먼트 파일과 이 파일의 내부 위치로 오프셋을 연관시킨다.
        - 인덱스도 세그먼트로 분할된다. 따라서 메시지가 삭제되면 그것과 연관된 인덱스 항목도 삭제할 수 있다. 카프카는 인덱스의 체크섬을 유지관리하지 않는다. 그리고 만일 인덱스가 손상되면 연관된 로그 세그먼트로부터 메시지들을 다시 읽고 오프셋과 위치를 수록하여 다시 생성한다. 필요하다면 관리자가 인덱스 세그먼트들을 삭제해도 전혀 문제가 없다. 인덱스 세그먼트들은 자동으로 다시 생성되기 때문이다.
    6. **압축**
        - 카프카는 설정된 시간동안 메시지들을 저장하며 보존기간이 지난 메시지들을 삭제한다. 예를 들어 카프카를 사용해서 고객의 배달 주소를 저장하는 경우를 생각해보자. 이때는 각 고객의 지난주나 작년의 주소가 아닌 마지막 ,즉 **가장 최근**의 주소를 저장하는 것이 바람직하다. 그리고 이렇게 하면 이전의 주소가 저장될 걱정을 할 필요가 없으며, 한동안 이사하지 않았던 고객의 주소도 그대로 보존된다. 또 다른 예로, 카프카를 사용해서 자신의 **현재 상태를 저장하는 애플리케이션**을 생각해보자. 매번 상태가 변경될 때마다 이 애플리케이션에서는 새로운 상태 데이터를 카프카에 쓴다. 그리고 문제가 생겨 종료되었다가 복구될 때 이 애플리케이션에서는 카프카에 저장된 상태 관련 메시지들을 읽어서 가장 최근 상태로 복구한다. 이 경우에는 **종료 직전의 가장 최근 상태에 관한 것 만이 관심의 대상**이며, 해당 애플리케이션이 실행 중에 발생한 변경 데이터들은 중요하지 않다.
        - 카프카는 토픽의 보존정책을 사용하여 앞에서 얘기한 사용 예를 모두 지원한다. 즉, **삭제 보존정책에서는 보존 기간 이전의 메시지들을 삭제**하며, **압축 보존정책에서는 각 키의 가장 최근 값만 토픽에 저장**할 수 있다. 그리고 **압축 보존정책은 키와 값을 갖는 메시지를 생성하는 애플리케이션의 토픽에만 적용**되며, **토픽에 null 키가 포함되면 압축이 안된다.**
        - 여기서 이야기하는 압축은 흔히 얘기하는 데이터압축과는 다른 개념이다. **데이터 압축은 알고리즘을 사용하여 전체 데이터의 용량을 줄이는 것이 목적이지만, 카프카의 메시지 압축은 같은 키를 갖는 메시지들이 토픽에 여러번 저장되었을때 키를 중심으로 이전 것은 삭제하고 가장 최근 것만 남기는 것을 말한다.**
    7. **압축 처리 방법**
        - **키와 값의 형태로 된 메시지를 수록하는 각 로그 세그먼트는 다음의 두 부분으로 나누어 생각할 수 있다.**
            1. **클린** (Clean) : **이전에 압축되었던 메시지**들이 있다. 이부분은 각 키에 대해 하나의 값만 포함하며, 이것은 이전에 압축할 당시의 가장 최근값이다.
            2. **더티** (Dirty) : **직전 압축 이후에 추가로 쓴 메시지들이 저장된 부분**이다.
        - **카프카가 시작될 때 압축이 활성화되면** (**log.cleaner.enabled 구성매개변수 설정 시**), **각 브로커는 하나의 압축 매니저 스레드와 여러 개의 압축 스레드를 시작시킨다**. 이 스레드들은 압축 작업을 수행하는 책임을 가지며, 각 스레드는 전체 파티션 크기보다 더티 메시지의 비율이 가장 큰 파티션을 선택하고 압축한다.
        - 파티션을 압축하기 위해, 압축 스레드는 파티션의 **더티 메시지들을 읽어서 메모리에 압축용 오프셋 Map을 생성**한다. 오프셋 Map의 각 항목은 키(16바이트의 해시 값)와 값(8바이트)으로 구성된다. (여기서 키는 메시지 키로 생성된 해시 값이며, 값은 해당 메시지의 오프셋이다.) 따라서 각 항목은 24바이트의 메모리만 사용한다. 예를 들어, 세그먼트 크기는 1GB이고 세그먼트의 각 메시지 크기는 1KB라고 가정한다면, 이 세그먼트는 100만개의 메시지를 갖는다. 따라서 이 세그먼트를 압축하기 위해 필요한 오프셋 Map은 최대 24MB면 된다. 그러나 실제로는 이보다 적을 수 있다. 같은 키를 갖는 메시지들이 많으면 메시지 키의 해시 값을 키로 사용하는 오프셋 Map의 항목 수도 적어질 것이기 때문이다.
        - **카프카를 구성할 때 관리자는 오프셋 Map에 사용할 메모리를 설정**한다. 이것은 **각 압축 스레드가 따로 가질 수 있는 오프셋 Map의 크기를 합한 전체 메모리**다. 예를 들어 오프셋 Map을 1GB로 구성하고 5개의 압축 스레드를 사용한다면, 각 스레드는 200MB의 Map을 따로 가질 수 있다. 또한 파티션의 더티 부분 전체가 Map에 다 들어가지 않아도 되지만, 최소한 하나의 세그먼트는 Map의 크기에 맞게 들어갈 수 있어야 한다. 그렇지 않으면 카프카가 에러를 로깅하므로, 관리자가 오프셋 Map의 메모리를 늘리거나 압축 스레드의 개수를 줄여서 사용해야한다. 만일 소수의 세그먼트만 Map에 들어갈 수 있으면, 카프카가 시작될 때 가장 오래된 세그먼트들부터 압축하여 Map에 넣을 것이다. 그외의 나머지 세그먼트들은 더티부분에 남아있다가 다음번 압축을 기다리게 된다.
        - **압축 스레드가 오프셋 Map을 생성한 다음에는 파티션의 클린 부분 세그먼트들을 가장 오래된 것부터 읽으면서 해당 세그먼트의 각 메시지 키가 오프셋 Map에 있는지 확인**한다. **만약 키가 오프셋 Map에 없으면 방금 읽은 메시지의 값이 가장 최근 것이므로 해당 메시지를 대체 세그먼트로 복사**한다. 그렇지 않고 **키가 오프셋 Map에 있으면 해당 메시지는 제외**한다. 왜냐하면 같은 메시지 키를 가진 **이후의 더 새로운 값이 파티션에 있기 때문**이다. 그리고 한 세그먼트의 모든 처리가 끝나면 원래 세그먼트를 대체 세그먼트로 교체하고 다음세그먼트를 계속 처리한다. 결국 **모든 작업이 끝나면 같은 키를 갖는 메시지는 하나씩만 남게되고 가장 최근 값을 갖게 된다.**
    8. **삭제된 메시지**
        - **가장 최근 메시지조차도 남기지 않고 시스템에서 특정 키를 완전히 삭제할 때는 애플리케이션에서 해당 키와 null 값을 포함하는 메시지를 카프카에 쓰면 된다.** 그러면 압축스레드에서 그런 메시지를 발견할 때 평상시대로 압축을 수행한 후 해당 키에 대해서는 null값을 갖는 메시지만 남겨둘 것이다. 그리고 **톰스톤**(**tombstone**)이라고 하는 이런 특별한 메시지는 카프카에 설정된 기간 동안 보존될 것이다. 또한 이 기간동안 컨슈머가 톰스톤 메시지를 읽으면 값이 null이라서 삭제되었음을 알 수 있으므로, 만일 이와 관련된 데이터를 관계형 데이터베이스에 저장한다면 거기에서 해당 사용자를 삭제해야 한다는 것도 알 수 있다. 그리고 카프카에 설정된 시간이 지나면 압축스레드에서 톰스톤 메시지를 삭제할 것이고, 해당 키의 메시지는 파티션에서 없어질 것이다.
        - 단, **이때는 컨슈머가 톰스톤 메시지를 인식하기에 충분한 시간을 주는 것이 중요하다**. 왜냐하면 만에 하나 컨슈머가 여러 시간동안 중단되어 톰스톤 메시지가 누락된다면 해당 키를 알지 못할 것이고, 이로 인해 카프카에서 삭제된 것인지 또는 컨슈머의 데이터베이스에서 삭제해야 하는지도 알수 없기 때문이다.
    9. **토픽의 압축 시기**
        - **현재 사용중인 액티브 세그먼트를 삭제하지 않는 삭제 보존정책과 마찬가지로, 압축 보존정책에서도 현재 사용중인 세그먼트는 압축하지 않는다.** 사용중이 아닌 세그먼트의 메시지들만 압축 대상이 된다.
        - 카프카 0.10.0 이하 버전에서는 토픽의 50%가 더티 레코드를 포함할 때 압축을 시작한다. 압축은 토픽의 읽고 쓰기 성능에 영향을 줄 수 있으므로 너무 자주하지 않는 것이 좋기 때문이다. 그러나 디스크 공간을 차지하므로 너무 많은 더티 레코드를 남겨두지 않는 것 또한 중요하다.
        - 따라서 더티 레코드가 토픽의 50%에 해당하는 디스크 공간을 사용할 때 압축을 하는 것이 합리적으로 보인다. 이 비율은 관리자가 조정할 수 있다.
    
----------------------------------------------------

## Reliable data delivery
1. **신뢰성 보장**
    - 가장 많이 알려진 신뢰성 보장은 ACID다. 이것은 관계형 데이터베이스가 보편적으로 지원하는 표준화된 신뢰성 보장이다.
        - 원자성(Atomicity), 일관성(Consistency), 고립성(isolation), 지속성(durabiliy)
    - 따라서 **어떤 데이터베이스가 ACID를 준수한다면 그것은 트랜잭션 처리와 관련하여 약속된 행동을 보장한다는 의미**가 된다.
    - **아파치 카프카가 제공하는 보장**
        1. **파티션의 메시지 순서를 보장**한다.
        2. **각 파티션의 모든 동기화 리플리카**(in-sync replica, **ISR**)에 메시지를 썼다면 **해당 메시지는 커밋된 것으로 간주**한다.
        3. **최소한 하나의 리플리카가 살아있다면 커밋된 메시지는 유실되지 않는다.**
        4. **컨슈머는 커밋된 메시지만 읽을 수 있다.**
    - 이런 기본적인 보장은 신뢰성 있는 시스템을 구축할 때 사용될 수 있지만, 그렇다고 해서 안전하게 신뢰성 있는 시스템을 만들어주는 것은 아니다. **신뢰성 있는 시스템을 구축할 때는 트레이드 오프가 수반된다.** (신뢰도가 높고 지속적으로 메시지를 저장하는 것의 중요도와 가용성, 높은 처리량, 낮은 지연 시간, 하드웨어 비용의 중요도 등)
2. **복제 메커니즘**
    - 파티션마다 다수의 리플리카를 가질 수 있는 **카프카의 복제 메커니즘은 신뢰성 보장의 핵심**이다. 장애 발생 시에 카프카가 메시지의 지속성을 제공하는 방법이 다수의 리플리카에 메시지를 쓰는 것이기 때문이다.
    - **리플리카가 파티션의 리더**이거나 **다음과 같은 팔로어 일때** 그 리플리카는 **동기화**되는 것으로 간주한다.
        1. **주키퍼와 세션이 연결되어 있음**. 즉, 최근 6초(설정 가능) 내에 주키퍼에게 **하트비트**를 전송했음을 의미한다.
        2. 최근 10초(설정 가능) 내에 **리더로부터 메세지를 읽었음**.
        3. 최근 10초 내에 **리더로부터 가장 최근 메시지를 읽었음**. 즉, **팔로어가 리더로부터 복제할 메시지를 읽는 것은 당연하고 지연된 메시지도 없어야 한다.**
    - 만일 리플리카가 **주키퍼와 연결이 끊겨서 새로운 메시지 읽기가 중단**되거나 **메시지 동기화에 뒤처지게 되어 10초 이내에 따라잡을 수 없다**면 **해당 리플리카는 동기화되지 않는 것으로 간주**한다.
    - 비동기화 리플리카는 주키퍼와 다시 연결되어 리더에 저장된 가장 최근 메시지들까지 복제하면 동기화 상태가 된다. 이런 일은 대개 일시적인 네트워크 문제가 발생했다가 바로 해결될 때 생길 수 있다. 그러나 만일 해당 리플리카의 브로커가 더 오랫동안 중단되었다면 더 많은 시간이 걸릴 수 있다.
    - **비동기 리플리카**
        - 하나 이상의 리플리카가 동기화와 비동기화 상태를 빠르게 오가는 것은 클러스터에 문제가 있다는 확실한 증거다. 이런 일은 브로커에서 자바의 가비지 컬렉션을 잘못 구성했을 때 종종발생한다. 가비지 컬렉션을 잘못 구성하면 브로커가 수 초 동안 중단되어 그 사이에 주키퍼와의 연결이 끊어질 수 있다. 이 경우 해당 브로커는 클러스터에서 비동기화 된 것으로 간주하므로 동기화와 비동기화 상태가 빠르게 전환되는 일이 생길 수 있다.
    - 동기화에 약간 뒤처진 동기화 리플리카는 프로듀서와 컨슈머의 처리 속도를 저하할 수 있다. 메시지가 커밋되기 전에 모든 동기화 리플리카의 메시지 수신을 기다리기 때문이다.
    - 그러나 **리플리카가 동기화되지 않는 상태가 되면 그것의 메시지 수신을 더 이상 기다리지 않는다. 이 경우 그 리플리카는 여전히 동기화가 뒤처지겠지만 프로듀서와 컨슈머의 처리 성능엔 영향을 주지 않는다.**
    - **만일 동기화 리플리카의 수가 더 적으면 라티션의 복제가 그만큼 덜 되는 것이므로 중단 시간이나 데이터 유실의 측면에서 위험이 증가**한다.
3. **브로커 구성**
    1. **신뢰성 있는 메시지 스토리지와 관련한 브로커의 구성 매개변수는 세 개**가 있다. 다른 많은 브로커 구성매개변수처럼 이 매개변수들도 브로커에 적용할 수 있으며, 시스템의 모든 토픽 구성을 제어한다. 신뢰성이 중요한 토픽과 신뢰성보다는 다른 관점이 더 중요한 토픽 모두를 같은 카프카 클러스터에 저장할 수 있다. **카프카에서는 토픽마다 신뢰성에 관련된 트레이드 오프를 제어할 수 있기 때문**이다.
    2. **복제 팩터**
        - 토픽 수준에서 복제 팩터를 구성하는 매개변수는 **replication.factor** 이며, 브로커 수준에서는 **자동 생성되는 토픽에 대해 default.replication.factor 매개변수를 설정**할 수 있다. (카프카의 기본 값은 3)
        - 이미 존재하는 토픽일지라도 리플리카를 추가하거나 삭제할 수 있으며, 이에 따라 복제 팩터도 수정할 수 있다.
        - 복제 팩터가 N 이면 N-1개의 브로커가 중단되더라도 여전히 토픽의 데이터를 신뢰성 있게 읽거나 쓸 수 있다. 따라서 복제 팩터가 클수록 가용성과 신뢰성은 높아지고 장애에 따른 데이터 유실은 적어진다.
        - 반면에 복제 팩터가 N일 경우 최소한 N개의 브로커가 필요하고, N개의 복사본을저장해야 하므로 N배의 디스크 공간이 필요하다.
        - 그러므로 **가용성을 높이는 대신 하드웨어가 더 많이 소요**된다.
        - 하나의 브로커가 다시 시작될 때 특정 토픽을 사용할수 없어도 괜찮다면 복제 팩터를 1로 해도된다. 이 경우 디스크나 서버를 절약할 수는 있지만 높은 가용성은 포기해야 한다
        - 복제팩터가 2라면 하나의 브로커가 중단되어도 문제가 없으므로 이렇게 해도 출분해 보이지만 하나의 브로커가 중단되면 클러스터가 불안정한 상태가 되어서 나머지 다른 브로커(카프카 컨트롤러)를 다시 시작해야 하는 경우가 생긴다. 즉, 복제 팩터를 2로 설정하면, 시스템 운영의 일환으로 브로커를 중단했다가 복구할 때 가용성을 보장할 수 없는 상태가 될 수 있다는 것이다. 따라서 이것은 위험부담이 큰 선택이다.
        - 이런 이유로 **가용성이 중요한 토픽은 복제 팩터를 3으로 설정할 것을 권한다**. (드물기는 하지만 이렇게 해도 충분하지 않은 경우가 있다)
        - **리플리카들의 위치도 매우 중요**하다. 기본적으로 카프카는 한 파티션의 각 리플리카들을 별개의 브로커에 둔다. 그러나 어떤 경우에는 이것이 충분히 안전하지 않다.
        - 만일 한 파티션의 모든 리플리카들이 같은 랙에 있는 브로커들에 속해 있는데 랙 상단의 스위치가 오동작한다면, 복제 팩터와는 무관하게 파티션의 가용성을 상실하게 될것이다.
        - 따라서 **랙 수준의 장애를 방지하기 위해 다수의 랙에 브로커들을 위치 시킨 후 브로커 구성 매개변수인 broker.rack을 사용해서 각 브로커의 랙 이름을 설정할 것을 권한다.**
        - 만일 랙 이름이 구성되면 훨씬 높은 가용성을 보장하기 위해 카프카는 파티션의 리플리카들이 다수의 랙에 걸쳐 분산되어 있는지 확인해준다.
    3. **언클린 리더 선출**
        - 이 구성은 브로커 수준(실제로는 클러스터)에서만 사용할 수 있다. 매개변수 이름은 **unclean.leader.election.enable** 이며 **기본 값은 true** 로 설정되어 있다.
        - **파티션의 리더를 더이상 사용할 수 없다면 동기화 리플리카 중 하나가 새로운 리더로 선출된다. 이 경우 커밋된 데이터가 유실되지 않는다는 것이 보장되므로 이것을 클린 리더 선출**이라고 한다. 여기서 커밋된 데이터란 모든 동기화 리플리카에 존재하는 데이터를 말한다.
        - 그러나 아래 두 경우에 사용할 수 없게 된 리더 외에는 동기화된 리플리카들이 아예 없게 된다.
            1. 파티션에 세 개의 리플리카가 있는데, 두 개의 팔로어를 사용할 수없게 되었을 때다. 이 경우 프로듀서가 리더에 계속 쓰는 동안 모든 메세지는 수신이 확인되고 커밋된다. 사용할 수 있는 동기화된 리플리카가 리더뿐이기 때문이다. 그다음에 리더를 사용할 수 없게 된다고 가정해보자. 그리고 비동기화 팔로어 중 하나가 먼저 시작된다면, 결국 해당 파티션의 사용할 수 있지만 유일한 리플리카로 비동기화 팔로어만 남게 된다.
            2. 파티션에 세 개의 리플리카가 있는데 네트워크에 문제가 생겨서 두 개의 팔로어가 복제에 뒤처지게 되었다. 그리고 여전히 사용할 수는 있지만 더 이상 동기화되지 않게 되었다. 이 경우 리더는 유일한 동기화 리플리카가 되어 메시지를 계속 받는다. 그 다음에 리더를 사용할 수 없게 된다면, 두 개의 사용가능한 리플리카들은 이후로도 계속 동기화될 수 없게 된다.
        - 두 개의 시나리오에서 다음의 신중한 판단을 내려야 한다.
            1. 만일 비동기화 리플리카를 새로운 리더가 될 수 없게 한다면, 해당 파티션은 이전 리더가 온라인이 될 때까지 오프라인 상태로 남게된다. 상황에 따라서는 수시간이 걸릴수도 있다.
            2. 만일 동기화 리플리카를 새로운 리더가 될 수 있게 한다면, 해당 리플리카가 동기화가 되지 않은 동안 이전에 리더에 썼던 모든 메시지들을 잃게 되고 컨슈머간에 일관성도 결여될 수 있다. 이것은 다운스트림 리포트와 같은 것을 살펴볼 때 데이터가 뒤섞여서 매우 나쁜 결과를 초래할 수 있다.
        - **만일 비동기화 리플리카를 리더가 될 수 있게 한다면, 데이터 유실과 일관성 결여의 위험을 감수해야 한다.** **그렇지 않고 리더가 될 수 없게 한다면, 가용성이 떨어지는 것에 직면하게 된다.** 즉 원래 리더를 사용할 수 있게 되어 해당 파티션이 다시 온라인 상태로 될 때까지 기다려야 한다.
        - **unclean.leader.election.enable을 true로 설정하는 것은 비동기 리플리카를 리더가 될 수 있게 한다**는 의미며, 이때는 메시지들이 유실될 수 있다. 그렇지않고 **false로 설정하면, 원래 리더가 온라인이 되기를 기다린다는 의미이므로 가용성이 떨어질 수 있다.** 일반적으로 **데이터 품질과 일관성이 중요한 시스템에서는 false로 설정**하여 언클린 리더 선출을 하지 못하게 한다. 이와는 달리 **가용성이 더 중요한 시스템에서는 true로 설정**하여 언클린 리더 선출을 할 수 있게 한다.
    4. **최소 동기화 리플리카**
        - 토픽과 브로커 모두의 구성에 관련되는 구성 매개변수로 **min.insync.replicas**가 있다.
        - 하나의 토픽이 세 개의 리플리카를 갖도록 구성되었더라도 동기화 리플리카는 하나만 남게 될 수 있다. 그리고 이 리플리카마저 사용할 수 없게된다면 가용성과 일관성 중 어느 쪽에 비중을 둘 지 선택해야 한다. 이것은 결코 쉬운일이 아니다. 카프카의 신뢰성 보장에서는 모든 동기화 리플리카에 메시지를 썼을 때 커밋된 것으로 간주하지만 하나만 남은 동기화 리플리카마저도 사용할 수 없게 되면 데이터가 유실될 수 있다는 문제가 있기 때문이다.
        - **커밋된 데이터를 하나 이상의 리플리카에 확실하게 쓰고자 한다면, 동기화 리플리카의 최소 개수를 더 큰 값으로 설정해야 한다.** 예를 들어 하나의 토픽이 세 개의 리플리카를 가질때 min.insync.replicas 매개변수르 2로 설정하면 세 개의 리플리카 중에서 최소 두 개가 동기화될 때 토픽이 파티션에 쓸 수 있다.
        - 이 경우 세개의 모든 리플리카가 동기화된다면 모든 것이 정상적으로 처리된다. 세 개 중 하나를 사용할 수 없게 되더라도 마찬가지다. 그러나 세 개의 리플리카 중 2개를 사용할 수 없게되면 브로커들이 더이상 쓰기 요청을 받지 않게 되며, 데이터를 쓰려고 하는 프로듀서는 NotEnoughReplicasException 예외를 받게 된다. **그러나 컨슈머는 기존 데이터를 계속 읽을 수 있다. 사실상, 이 경우에는 하나의 동기화 리플리카가 읽기 전용이 되는 셈**이며, 그럼으로써 바람직하지 않은 데이터를 읽고 쓰는 것을 방지하고 언클린 리더가 선출되지 않게 한다. 그리고 이런 읽기 전용 상황을 원래대로 복구하기 위해서는 사용 불가능한 두 개의 리플리카 중 하나를 다시 사용할 수 있게 한후(해당 브로커를 다시시작) 그 동안 밀렸던 메시지를 처리하고 동기화되도록 해야 한다.
4. **신뢰성 있는 시스템에서 프로듀서 사용하기**
    1. 가장 신뢰성이 높게 브로커를 구성하더라도 프로듀서 역시 신뢰성있게 구성하지 않는다면 시스템 전체로는 여전히 데이터가 유실될 수 있다. 메시지를 쓰는 모든 **프로듀서는 두 가지에 주의**해야 한다.
        1. **신뢰성 요구 사항에 맞도록 acks 구성 매개변수를 올바르게 설정**해야 한다.
        2. **구성 매개변수와 코드 모두에서 에러처리**를 올바르게 해야한다.
    2. **확인 응답 전송** (Producer Configuration 참고)
        - **acks 구성 매개변수**에 0,1,all 중 하나를 설정
    3. **프로듀서의 재시도 구성**하기
        - 프로듀서의 에러처리에는 두 가지가 있다.
            1. **프로듀서가 자동으로 처리하는 에러**
            2. **개발자가 프로듀서 라이브러리를 사용해서 처리해야 하는 에러**
        - 프로듀서는 브로커가 반환하는 재시도 가능한 에러를 처리할 수 있다. 프로듀서가 브로커에게 메시지를 전송하면 브로커는 성공 또는 에러 코드를 반환한다. 이런 에러코드는 두 가지 부류에 속한다. 전송을 재시도 하면 해결될 수 있는 것과 해결 안되는 에러다.
        - 예를 들어 브로커가 LEADER_NOT_AVAILABLE 에러 코드를 반환하면 프로듀서는 메시지 전송을 다시 시도할 수 있으며 이 경우 새로운 브로커가 리더로 선출되었을 것이므로 두번째 시도는 성공할 것이다. 즉, LEADER_NOT_ABAILABLE은 재시도 가능한 에러라는 의미다.
        - 이와는 달리 브로커가 INVALID_CONFIG 예외를 반환하면 같은 메시지를 다시 전송해도 구성이 변경되지 않는다. 이것은 재시도 불가능한 에러의 예이다.
        - 만일 절대로 메시지가 유실되지 않는 것을 원한다면, 프로듀서가 재시도 가능한 에러를 접했을 때 메시지 재전송을 계속하도록 프로듀서를 구성하는 것이 가장 좋은 방법이다. 리더가 없거나 네트워크 연결로 인해 생기는 문제들은 수 초가 지나면 해결되는 것이기 때문이다. 그리고 카프카의 프로듀서 객체가 알아서 해주므로 우리 코드에서 직접 처리할 필요가 없다. 그렇다면 몇번을 재시도하도록 프로듀서를 구성해야 할까? 이에 대한 답은 프로듀서가 N번 재시도하고 포기한 후 우리가 하고자하는 것에 달려있다. 만일 예외 발생시 몇 번 더 재시도하겠다라고 한다면, 재시도 횟수를 더 큰 값으로 설정하여 프로듀서가 계속 재시도하도록 해야한다. 그렇지 않고 재시도를 해봐야 소용없으니 메시지를 바로 삭제하겠다 또는 다른 곳에 메시지를 저장하고 나중에 처리하겠다라고 한다면 재시도를 중단하면 된다.
        - **카프카의 크로스 데이터센터 통신**(Cross Datacenter Communication) 복제도구인 **미러메이커** (MirrorMaker)에는 **무한정 재시도**(**retries = MAX_INT**)가 기본값으로 구성되어 있다. 이것은 높은 신뢰성을 갖는 복제도구 이므로 메시지를 삭제하면 안되기 때문이다.
        - 전송에 실패한 메시지의 재전송 시도에는 사소한 위험이 생기는 경우가 있다는 것에 유의하자. 예를 들어 브로커에서 성공적으로 메시지를 쓰고 복제된 후에 브로커가 프로듀서에게 보내는 확인 응답이 네트워크 문제로 전송되지 않았다면 프로듀서는 일시적인 네트워크 문제 때문에 확인 응답이 없는 것으로 간주하고 해당 메시지를 다시 전송할 것이다. 브로커가 받은 것을 알 수 없기 때문이다. 이경우 브로커는 같은 메시지를 두 번 갖게 된다. 메시지 재전송시도와 신중한 에러처리는 각 메시지가 최소 한번 저장되는 것을 보장할 수 있다. 그러나 아파치 카프카 0.10.0 버전을 기준으로 정확히 한번만 저장된다는 것이 보장되지 않는다. 따라서 실제 애플리케이션에서는 각 메시지에 고유 식별자를 추가하여 중복을 찾아내고 메시지를 읽을 때 걸러내도록 하는 경우가 많다. **idempotent** 메시지로 만들어 사용하는 애플리케이션도 있다. (idempotent는 같은 요청을 여러번해도 결과는 매번 같다는 의미다.)
    4. **추가적인 에러 처리**
        - 카프카 프로듀서 객체가 자동으로 해주는 재시도를 사용하면 메시지 유실 없이 다양한 에러를 올바르고 쉽게 처리해준다. 그러나 여전히 개발자가 처리해야 하는 에러가 있다.
            1. 메시지 크기, 인증 에러 등과 같이 재시도 불가능한 브로커 에러
            2. 메시지가 브로커에게 전송되기 전에 발생한 에러 (예를 들어, 직렬화 에러)
            3. 구성 매개변수에 지정된 횟수만큼 프로듀서가 재시도했을때 또는 전송을 재시도하는 동안 메시지들을 저장하는 것 때문에 프로듀서가 사용할 수 있는 메모리가 꽉 찼을때 발생하는 에러
5. **신뢰성 있는 시스템에서 컨슈머 사용하기**
    1. **데이터는 카프카에 커밋된 후에만 컨슈머가 읽을 수 있다. 따라서 컨슈머는 일관성이 보장된 데이터를 읽는다. 그리고 컨슈머는 읽은 메시지와 읽지 않은 메시지를 계속 파악만 하면 된다. 이것이 메시지를 읽는 동안 누락시키지 않는 방법이다.**
        1. 파티션의 데이터를 읽을 때 컨슈머는 메시지 배치를 읽고 처리한 후 해당 배치의 마지막 오프셋을 확인하여 그 오프셋부터 시작하는 다른 메시지 배치의 읽기를 요청한다. 따라서 메시지의 누락없이 카프카 컨슈머가 항상 올바른 순서로 새로운 데이터를 읽게 해준다.
        2. 특정 컨슈머가 중단되면 어디서부터 계속 읽어야 할지 다른 컨슈머가 알아야 한다. 즉 이전 컨슈머가 중단되기 전에 읽고 처리했던 파티션의 마지막 오프셋을 알아야 한다는 의미다. 이때 다른 컨슈머는 다시 시작된 원래의 컨슈머가 될수도 있지만 그것은 중요하지 않다. 어떤 컨슈머가 해당 파티션을 읽더라도 시작할 오프셋을 알아야 하기 때문이다. 컨슈머가 자신의 오프셋을 커밋해야하는 이유가 바로 그 때문이다. 각 파티션을 읽을 때 컨슈머는 현재 위치를 저장한다. 따라서 자신 또는 다른 컨슈머가 다시 시작된 후 어디서부터 계속 읽을지 알 게 된다. 주로 컨슈머는 다음의 경우에 메시지를 누락 시킬 수 있다. 즉, 이미 읽었지만 아직 완전히 처리되지 않은 메세지의 오프셋을 커밋할 때다. 이 경우 다른 컨슈머가 이어서 읽을 때 이 메시지들을 읽지 않게 되고 처리도 되지 않는다. 언제 그리고 어떻게 커밋하느냐가 중요한 이유가 바로 그 때문이다.
            - **커밋된 메시지는 프로듀서가 전송하고 모든 동기화 리플리카에 썼으므로 컨슈머가 읽을 수 있는 메시지다. 그리고 컨슈머가 파티션의 특정 오프셋까지의 메시지를 수신하고 처리했다는것을 알리기 위해 카프카에 전송하는 오프셋이 커밋된 오프셋이다.**
    2. **신뢰성 있는 처리에 중요한 컨슈머 구성 속성** (매개 변수)
        1. **group.id**
            - 만일 두 개의 컨슈머가 같은 그룹 ID를 갖고 같은 토픽을 구독한다면, 각 컨슈머는 해당 토픽의 일부 파티션을 분담하므로 할당받은 파티션의 메시지만 읽게된다.
            2. 만일 구독하는 토픽의 모든 메시지를 하나의 컨슈머가 읽어야 한다면 별도의 고유한 그룹 ID를 설정하고 사용하면 된다.
        2. **auto.offset.reset**
            - 커밋된 오프셋이 없을 때 또는 브로커에 없는 오프셋을 컨슈머가 요청할 때 컨슈머가 할일을 제어하는 매개변수
            - earliest : 해당 파티션의 맨 앞부터 모든 데이터를 읽는다. (중복 가능, 누락 최소화)
            - latest : 해당 파티션의 제일 끝부터 읽기 시작한다. (중복 최소화, 누락 가능성이 크다.)
        3. **enable.auto.commit**
            - 컨슈머의 오프셋 커밋을 자동으로 할 것인지 (기본 값, true), 또는 코드에서 직접 할 것인지 (false) 결정
        4. **auto.commit.interval.ms**
            - **enable.auto.commit이 true일 때** 자동으로 오프셋을 커밋하는 시간 간격 제어 (기본값 5초)
            - 오프셋 자동 커밋을 자주 하도록 시간을 설정하면 약간의 부담이 따르지만 컨슈머의 중단으로 인해 초래될 수 있는 중복 메시지의 수를 줄일 수 있다.
    3. **컨슈머에서 오프셋 커밋하기**
        1. 자동 오프셋 커밋을 사용한다면 오프셋 커밋을 신경쓰지 않아도 된다. 그러나 오프셋을 커밋하는 시점 외에 다른 것도 제어해야한다면 어떻게 할 것인지 신중하게 생각해야 한다. 예를 들어 중복 메시지를 최소화하거나 컨슈머의 폴링 루프 밖에서 메시지를 처리하는 경우다.
        2. **고려사항**
            1. **오프셋 커밋은 항상 메시지가 처리된 후에 해야한다.**
                - 만일 컨슈머의 폴링 루프 안에서 모든 처리를 하고 촐링 루프가 반복될 때마다 상태 정보를 유지하지 않는다면 이것은 쉽다. 즉, 자동 오프셋 커밋을 사용하도록 구성하거나 폴링루프의 끝에서 커밋하면 된다.
            2. **오프셋 커밋 빈도는 성능과 중복 메시지 개수 간의 트레이드 오프다.**
                - 컨슈머의 폴링 루프안에서 모든 처리를 하고 폴링 루프가 반복될 때마다 상태 데이터를 유지하지 않는 가장 간단한 경우일지라도, 폴링 루프 내부에서 여러번 커밋하거나 또는 몇 차례 반복할 때만 커밋할 수도 있다.
                - 오프셋 커밋은 성능에 약간 부담을 준다(프로듀서에서 acks=all을 설정하는 것과 유사). 따라서 컨슈머의 중단으로 인해 초래될 수있는 중복 메시지의 수를 줄이는 것과 처리 성능 중에서 어느 것에 더 비중을 둘지에 따라 커밋 빈도를 결정해야 한다.
            3. **어떤 오프셋을 커밋하는지 정확하게 알아야 한다.**
                - 컨슈머의 폴링 루프 중간에서 커밋할 때는 처리된 마지막 오프셋이 아닌 읽기만한 마지막 오프셋을 커밋하는 오류를 범할 수 있다. 항상 처리된 메시지의 오프셋을 커밋하는 것이 중요하다. 읽기만 하고 처리는 안 된 메시지의 오프셋을 커밋하면 컨슈머가 메시지를 누락시킬 수 있기 때문이다.
            4. **리밸런싱**
                - 애플리케이션을 설계할 때는 컨슈머 리밸런싱이 수행될 수 있고 그것을 적합하게 처리해야 한다는 것을 기억하자. 컨슈머 리밸런싱이 시작되기 전에 마지막 처리된 메시지의 오프셋을 커밋하는 등의 **클린업** 처리를 해야한다. 메시지의 중복 처리를 방지하기 위해서다.
            5. **컨슈머는 상태 데이터를 유지해야 한다.**
                - 애플리케이션에 따라서는 poll() 메서드 호출 간에 상태 데이터를 유지해야 할 경우가 있다. 예를 들어 이동 평균을 계산하고자 한다면, poll() 메서드를 호출하여 새로운 메시지를 읽을 때마다 평균값을 변경해야 한다. 그러나 만일 이런 처리가 다시 시작된다면, 마지막 오프셋부터 메시지를 읽기 시작해야 하는 것을 물론이고 이전에 보존했던 이동 평균값도 복구해야 한다. 이때는 다음의 방법이 있다.
                - 오프셋을 커밋하는 동시에 가장 최근의 누적 값을 특정 토픽에 쓰는 것이 한가지 방법이다. 이렇게 하면 컨슈머 스레드가 시작될 때 마지막 누적값을 찾을 수 있다. 그러나 카프카는 아직 트랜잭션을 제공하지 않으므로 이렇게해도 문제가 완전히 해결되지는 않는다. 왜냐하면 마지막 누적값을 쓰고 오프셋을 커밋하기 전에, 또는 그 반대의 경우에서 컨슈머가 중단될 수 있기 때문이다.
                - 이것은 해결하기가 다소 복잡한 문제이므로, 자체적으로 해결하기보다는 카프카 스트림즈(Kafka Streams)와 같은 라이브러리를 찾아보기를 권한다. 카프카 스트림즈는 고수준의 DSL API를 제공하며, 카프카에 저장된 데이터를 처리하고 분석하는 클라이언트 라이브러리다.
            6. **긴 처리 시간에 대처해야 한다.**
                - 카프카에서 읽은 레코드 처리에 시간이 오래 걸릴 때가 있다. 카프카 일부 버전에서는 수초 이상 폴링을 중단할 수 없다는 것을 알아두자. 심지어는 레코드를 추가로 읽고 싶지 않더라도 폴링을 계속해야 한다. 그래야만 컨슈머 클라이언트에서 브로커에게 하트비트를 전송하여 살아있다는 것을 알릴 수 있기 때문이다. 이런 경우 레코드를 스레드 풀에 전달하여 처리하게 하는 것이 흔히 사용하는 방법이다. 스레드 풀을 사용하면 처리 속도를 약간 높이기 위해 병행 처리로 다수의 스레드를 사용할 수 있기 때문이다. 그리고 컨슈머에서는 스레드 풀의 작업 스레드들에게 레코드들을 전달한 후 레코드 처리는 하지 않고 폴링만 계속 하면서 작업스레드들이 완료될 때까지 기다리면 된다. 그 다음에 작업 스레드들이 완료된 후에 데이터 읽기를 재개하면 된다. 이 경우 컨슈머가 폴링을 중단하지 않으므로 하트비트가 정상적으로 전송되어 리밸런싱도 수행되지 않는다.
            7. **Exactly-once 전송**
                - **카프카는 at-least-once 전송은 지원하지만 exactly-once 전송은 완벽하게 지원되지 않는다.**
                - exactly-once 전송을 하기 위해 가장 많이 사용되는 방법은 고유한 키를 지원하는 외부 시스템에 식별 데이터를 쓰는 것이다. 이때 키-값 데이터스토어, 모든 관계형 데이터베이스, 엘라스틱 서치 등의 시스템을 사용할 수 있다. 그리고 이런 외부 시스템에 식별 데이터를 쓸 때 카프카 레코드 자체에 고유한 키를 포함하거나 또는 토픽과 파티션 및 오프셋을 조합하여 고유한 키를 생성하면 된다. 그 다음에 해당 키와 값으로 된 식별데이터를 외부 시스템에 쓰면, 나중에 프로듀서에서 레코드를 쓰거나 컨슈머에서 읽을 때 그것을 확인하여 레코드의 중복을 방지할 수 있다. 이것을 **idempotent 쓰기**라고 하며 흔히 사용되는 유용한 방법이다.
                - 이외에도 트랜잭션 처리가 가능한 외부 시스템에 쓰는 방법이 있다. 이런 외부 시스템의 가장 쉬운 예는 관계형 데이터베이스이며, 하둡 분산 파일 시스템(HDFS, Hadoop Distributed File System)도 흔히 사용된다. 이 방법에서는 같은 트랜잭션에 카프카 레코드와 그것의 오프셋을 써서 동기화되도록 한다. 그리고 외부 시스템에 쓴 가장 최근 레코드의 오프셋을 읽은 후 카프카에서 consumer.seek() 를 호출하여 해당 오프셋부터 레코드를 읽으면 된다.
6. **시스템 신뢰성 검사하기**
    1. **구성 검사**
        - 카프카는 구성 검사에 도움이 되는 두 개의 중요한 도구를 포함하고 있다.
        - **org.apache.kafka.tools 패키지에 있는 VerifiableProducer 와 VerifiableConsumer**, 이 클래스들은 명령행 도구 또는 자동화 테스트 프레임워크에 포함시켜 실행시킬 수 있다.
        - VerifiableProducer는 1부터 우리가 선택한 값까지의 숫자를 포함하는 메시지들을 카프카에 쓴다. 이때 프로듀서를 구성할 때와 같은 방법으로 acks, retries 구성 매개변수의 적합한 값을 설정하여 그 프로듀서를 구성할 수 있다. 그리고 그것을 실행하면 acks에 근거하여 브로커에 전송된 각 메시지의 성공 또는 에러를 출력한다. VerifiableConsumer는 상호 보완적인 검사를 수행한다. 즉, 메시지를 읽은 후 그 순서대로 출력한다. 또한 커밋과 리밸런싱에 관련된 정보도 출력한다.
        - 테스트 시나리오 예
            1. **리더 선출** : 리더를 중단시키면 어떻게 될까? 프로듀서와 컨슈머가 평상시퍼럼 다시 작동을 시작하는 데 얼마나 걸릴까?
            2. **컨트롤러 선출** : 컨트롤러를 다시 시작시킨 후 시스템이 재개되는데 얼마나 걸릴까?
            3. **단계적 재시작** : 메시지 유실없이 브로커를 하나씩 재시작시킬 수 있을까?
            4. **언클린 리더 선출 테스트** : 한 파티션의 모든 리플리카들을 하나씩 중단시킨 다음에 비동기화되었던 브로커를 시작시키면 어떻게 될까?
        - 업그레이드 작업을 단계적으로 하기 위해 사용한다.
    2. **애플리케이션 검사**
        - 브로커와 클라이언트의 구성이 요구사항을 충족시키는 것을 확인한 다음에는 애플리케이션에서 우리에게 필요한 신뢰성 보장을 제공하는지 테스트한다. 이때는 에러처리코드, 오프셋커밋, 리밸런싱 리스너, 그리고 카프카의 클라이언트 라이브러리와 상호작용하는 애플리케이션 로직 등을 확인한다.
        - 테스트 시나리오 예
            1. 클라이언트와 서버간의 연결끊어짐
            2. 리더 선출
            3. 브로커들을 중단시킨 후 하나씩 다시 시작함
            4. 컨슈머들을 중단시킨 후 하나씩 다시 시작함
            5. 프로듀서들을 중단시킨 후 하나씩 다시 시작함.
    3. **실제 업무 운용 시의 신뢰성 모니터링하기**
        - **클러스터의 모니터링뿐만 아니라 시스템을 통한 클라이언트와 데이터의 이동을 모니터링하는 것도 중요하다.**
        - 카프카의 자바 클라이언트는 JMX 메트릭을 포함한다. 이것은 클라이언트 측의 상태와 메시지를 모니터링할 수 있게 해준다.
        - 프로듀서의 경우에는 두 개의 메트릭(레코드당 에러율과 재시도율)이 신뢰성 테스트에 가장 중요하다. 에러율과 재시도율이 올라가면 시스템에 문제가 있음을 나타내는 것이므로 잘 지켜봐야 한다. 또한 프로듀서 로그에 에러가 없는지도 지켜봐야 한다. 이런 에러는 메시지를 전송할 때 발생하며, WARN 레벨의 에러로 로그에 수록된다. 만일 재시도 횟수가 0이 되면 프로듀서가 쓰기 재시도를 할 수 없게 되므로, retries 매개변수 값을 증가시키거나 에러를 발생시킨 문제점을 찾아 먼저 해결해야 한다.
        - 컨슈머 측면에서 가장 중요한 메트릭은 컨슈머의 처리지연(lag)이다. 브로커의 파티션에 마지막으로 커밋된 메시지로부터 얼마나 뒤쳐져있는지를 나타내는 것이 이 메트릭이다. 이상적으로는 지연이 항상 0이어야 하고 컨슈머는 항상 가장 최근의 메시지를 읽어야 한다.
        - 그러나 실제로는 그렇지 않다. 왜냐하면 poll() 메소드를 호출하면 여러 개의 메시지가 반환되고 그 다음에 컨슈머는 추가로 메시지를 읽기 전에, 읽은 메시지들을 처리하느라 시간을 소비하므로 항상 약간의 지연이 생길 수 있기 때문이다. 이때 중요한 것은 컨슈머가 점점 더 뒤떨어지지 않고 지연된 메시지들을 따라잡게 하는 것이다. 그러나 컨슈머의 지연은 수시로 변동되므로 파악하기 어렵다. 이때 링크드인의 버로우(Burrow)를 사용하면 컨슈머 지연 검사를 더 쉽게 할 수 있다.
        - 데이터의 이동을 모니터링하는 것은 프로듀서가 쓴 모든 데이터를 시기적절하게 컨슈머가 읽는지 확인하는 것을 의미한다. 데이터가 시기적절하게 읽히는지 확인하기 위해서는 데이터를 언제 썼는지 알아야 한다. 이를 위해 카프카 0.10.0 버전부터는 메시지를 언제 썼는지 나타내는 타임스탬프가 모든 메시지에 포함된다. 만일 0.10.0 이전 버전의 클라이언트를 실행한다면, 타임스탬프, 메시지를 쓴 애플리케이션 이름, 메시지가 생성된 호스트 이름을 각 메시지에 수록하기를 권한다. 이렇게 하면 나중에 문제의 근원을 찾는 데 도움이 된다.
        - 카프카에 썼던 모든 메시지를 적절한 시간 안에 읽는지 확인하기 위해서는 썼던 메시지의 수를 기록하는 애플리케이션 코드가 필요하다. 또한 읽은 메시지 수와 메시지를 썼던 시간부터 읽은 시간까지의 지연 시간 모두를 타임스탬프를 사용해서 기록하는 컨슈머도 있어야 한다. 그리고 프로듀서와 컨슈머 모두로부터 쓰고 읽었던 초당 메시지 수를 대조하고, 적절한 시간 안에 썼던 메시지들간의 시간 간격을 확인하는 시스템이 필요하다. 이외에도 더 향상된 모니터링을 위해서 중요한 토픽들을 모니터링하는 컨슈머를 추가할 수도 있다. 이 컨슈머는 읽은 메시지 수를 계산하여 썼던 메시지 수와 비교하므로 프로듀서의 정확한 모니터링을 할 수 있다. 이런 유형의 모니터링 시스템들은 쉽지않으므로 구현하는 데 시간이 오래 걸릴 수 있다.

----------------------------------------------------

## Build the data pipeline


----------------------------------------------------

## Cross-cluster data mirroring


----------------------------------------------------

## Kafka management


----------------------------------------------------

## Kafka monitoring


----------------------------------------------------

## Stream processing


----------------------------------------------------

## Simple Projects
- **Detailed Source Code** : can be found in each directory.  

1. **simple-kafka-producer** : key = null, topic만 사용하여 value 전송
2. **Kafka-producer-key-value** : topic, key, value를 사용하여 전송
3. **Kafka-producer-exact-partition** : topic, partition, key, value를 사용하여 전송
4. **simple-kafka-consumer** : topic과 consumer group을 사용하여 데이터 읽기
5. **Kafka-consumer-auto-commit** : 일정 시간, polling 할 때 자동으로 commit
6. **Kafka-consumer-sync-commit** : commitSync(), commitAsync()를 사용하여 commit
7. **Kafka-consumer-multi-thread** : multi thread를 이용하여 파티션이 끊어지거나 새로 할당되는 등의 상황에 안전하게 종료하기
8. **Kafka-consumer-save-metric** : telegraf를 통해 데이터를 수집하고 카프카 적재 후 읽어와서 .csv 형식으로 저장
    1. **Homebrew** [설치](https://www.whatwant.com/entry/LinuxBrew-install-Ubuntu-1804)
    2. **Telegraf** 설치
        ```bash
        brew install telegraf
        ```
    3. **Telegraf 설정 파일 생성 및 설정**
        1. telegraf 설치 경로 확인
        ```bash
        brew info telegraf
        ```
        2. **telegraf.conf 파일 생성**
        ```bash
        cd {telegraf 설치경로}/bin
        vi telegraf.conf
        ```
        3. **telegraf.conf 파일 설정**
        ```html
        [agent]
          interval = "10s"
        [[outputs.kafka]]
          brokers = ["{AWS EC2 Public IP}:9092"]
          ## Kafka topic for producer messages
          topic = "my-computer-metric"
        [[inputs.cpu]]
          percpu = true
          totalcpu = true
          fielddrop = ["time_*"]
        [[inputs.mem]]
        ```
    4. **카프카 토픽 생성**
        ```bash
        (KAFKA_HOME)/bin/kafka-topics.sh --create --bootstrap-server {AWS EC2 Public IP}:9092 --replication-factor 1 --partitions 5 --topic my-computer-metric
        ```
    5. **Telegraf 실행** (telegraf 설치경로/bin)
        ```bash
        ./telegraf --config telegraf.conf
        ```
    5. 데이터 확인
        ```bash
        (KAFKA_HOME)/bin/kafka-console-consumer.sh --bootstrap-server {AWS EC2 Public IP}:9092 --topic my-computer-metric --from-beginning
        ```
    6. **자바 프로젝트 실행(Kafka-consumer-save-metric)** 후 파일 확인
        ```bash
        (자바 프로젝트 위치) tail -f *.csv
        ```
    7. 화면 캡쳐
        <p align="center">
            <img src="Images/brew1.png",width="100%">
        </p>
        <p align="center">
            <img src="Images/brew2.png",width="100%">
        </p>